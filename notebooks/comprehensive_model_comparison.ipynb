{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Comprehensive Model Comparison: HistoryCentricModel vs. Baseline Models\\n",
        "\\n",
        "## Overview\\n",
        "\\n",
        "This notebook provides a **comprehensive, fair comparison** of the **HistoryCentricModel** against several baseline models for next-location prediction on the GeoLife dataset. The goal is to evaluate each model under identical conditions to understand their relative strengths and weaknesses.\\n",
        "\\n",
        "### Models Compared:\\n",
        "\\n",
        "1. **HistoryCentricModel** - The proposed model that combines history-based scoring with learned patterns\\n",
        "2. **Transformer-Only** - Pure transformer architecture without history priors\\n",
        "3. **LSTM** - Traditional recurrent neural network with LSTM cells\\n",
        "4. **GRU** - Gated Recurrent Unit variant\\n",
        "5. **RNN** - Simple recurrent neural network\\n",
        "6. **Markov Chain** - First-order Markov model based on transition probabilities\\n",
        "7. **Frequency Baseline** - Predicts based on most frequently visited locations\\n",
        "\\n",
        "### Fair Comparison Criteria:\\n",
        "\\n",
        "- **Same Dataset**: All models use identical train/validation/test splits from GeoLife\\n",
        "- **Same Features**: All models receive the same input features (location, user, temporal info)\\n",
        "- **Similar Model Size**: Models are configured to have comparable parameter counts (~100K-200K parameters)\\n",
        "- **Same Training Setup**: Identical batch size, learning rate schedule, early stopping, and evaluation metrics\\n",
        "- **Same Evaluation**: All models evaluated on Acc@1, Acc@5, Acc@10, MRR, NDCG, and F1 score\\n",
        "\\n",
        "### Notebook Structure:\\n",
        "\\n",
        "1. **Setup and Data Loading** - Import libraries, load preprocessed data\\n",
        "2. **Model Implementations** - Self-contained implementations of all models\\n",
        "3. **Training Infrastructure** - Shared training loop and evaluation functions\\n",
        "4. **Model Training** - Train each model with identical hyperparameters\\n",
        "5. **Results Analysis** - Compare performance metrics and visualize results\\n",
        "6. **Discussion** - Interpret findings and insights\\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Imports\\n",
        "\\n",
        "We begin by importing all necessary libraries. This notebook is **self-contained** and does not depend on any external project scripts. All model implementations and utilities are defined within this notebook.\\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Core libraries\\n",
        "import pickle\\n",
        "import numpy as np\\n",
        "import pandas as pd\\n",
        "import matplotlib.pyplot as plt\\n",
        "import seaborn as sns\\n",
        "from collections import defaultdict, Counter\\n",
        "import warnings\\n",
        "warnings.filterwarnings('ignore')\\n",
        "\\n",
        "# PyTorch\\n",
        "import torch\\n",
        "import torch.nn as nn\\n",
        "import torch.nn.functional as F\\n",
        "from torch.utils.data import Dataset, DataLoader\\n",
        "from torch.optim import AdamW\\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\\n",
        "\\n",
        "# Scikit-learn for metrics\\n",
        "from sklearn.metrics import f1_score\\n",
        "\\n",
        "# Standard libraries\\n",
        "import time\\n",
        "import math\\n",
        "from pathlib import Path\\n",
        "import os\\n",
        "\\n",
        "# Set random seeds for reproducibility\\n",
        "SEED = 42\\n",
        "np.random.seed(SEED)\\n",
        "torch.manual_seed(SEED)\\n",
        "if torch.cuda.is_available():\\n",
        "    torch.cuda.manual_seed(SEED)\\n",
        "    torch.cuda.manual_seed_all(SEED)\\n",
        "    torch.backends.cudnn.deterministic = True\\n",
        "    torch.backends.cudnn.benchmark = False\\n",
        "\\n",
        "# Device configuration\\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\\n",
        "print(f'Using device: {device}')\\n",
        "if torch.cuda.is_available():\\n",
        "    print(f'GPU: {torch.cuda.get_device_name(0)}')\\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Configuration\\n",
        "\\n",
        "We define a unified configuration that will be used across all models to ensure fair comparison. All models will:\\n",
        "- Use the same batch size and sequence length\\n",
        "- Be trained for the same number of epochs with the same early stopping patience\\n",
        "- Use the same learning rate and optimization settings\\n",
        "- Have similar model capacity (controlled by embedding dimensions and hidden sizes)\\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Config:\\n",
        "    \\\"\\\"\\\"Unified configuration for fair model comparison\\\"\\\"\\\"\\n",
        "    \\n",
        "    # Data paths - UPDATE THESE PATHS TO MATCH YOUR SETUP\\n",
        "    data_dir = '../data/geolife'\\n",
        "    train_file = 'geolife_transformer_7_train.pk'\\n",
        "    val_file = 'geolife_transformer_7_validation.pk'\\n",
        "    test_file = 'geolife_transformer_7_test.pk'\\n",
        "    \\n",
        "    # Dataset parameters\\n",
        "    num_locations = 1187  # 1186 unique + 1 for padding (0)\\n",
        "    num_users = 46  # 45 unique + 1 for padding (0)\\n",
        "    num_weekdays = 7\\n",
        "    \\n",
        "    # Model capacity (same for all models for fair comparison)\\n",
        "    loc_emb_dim = 64\\n",
        "    user_emb_dim = 16\\n",
        "    hidden_dim = 128  # Hidden dimension for RNN/LSTM/GRU\\n",
        "    d_model = 128  # For Transformer\\n",
        "    nhead = 4  # For Transformer\\n",
        "    num_layers = 2  # Number of layers for stacked models\\n",
        "    dropout = 0.3\\n",
        "    \\n",
        "    # Sequence parameters\\n",
        "    max_seq_len = 60\\n",
        "    \\n",
        "    # Training parameters\\n",
        "    batch_size = 96\\n",
        "    num_epochs = 50  # Reduced for notebook execution, increase to 120 for full training\\n",
        "    learning_rate = 0.001\\n",
        "    weight_decay = 1e-4\\n",
        "    grad_clip = 1.0\\n",
        "    \\n",
        "    # Scheduler\\n",
        "    scheduler_patience = 10\\n",
        "    scheduler_factor = 0.5\\n",
        "    min_lr = 1e-6\\n",
        "    \\n",
        "    # Early stopping\\n",
        "    early_stop_patience = 15\\n",
        "    \\n",
        "    # Logging\\n",
        "    log_interval = 50\\n",
        "    \\n",
        "    # Device\\n",
        "    device = device\\n",
        "\\n",
        "config = Config()\\n",
        "print(\\\"Configuration loaded successfully\\\")\\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Dataset and DataLoader\\n",
        "\\n",
        "We implement a PyTorch Dataset class to load the GeoLife preprocessed data. The data contains trajectory sequences with:\\n",
        "- **Location IDs** (X): Sequence of visited locations\\n",
        "- **User IDs** (user_X): User identifier for each visit\\n",
        "- **Temporal features**: Weekday, start time (minutes from midnight), duration, time gap\\n",
        "- **Target** (Y): Next location to predict\\n",
        "\\n",
        "The collate function handles variable-length sequences by padding them to the maximum length in each batch.\\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class GeoLifeDataset(Dataset):\\n",
        "    \\\"\\\"\\\"Dataset for GeoLife trajectory sequences\\\"\\\"\\\"\\n",
        "    \\n",
        "    def __init__(self, data_path, max_seq_len=60):\\n",
        "        with open(data_path, 'rb') as f:\\n",
        "            self.data = pickle.load(f)\\n",
        "        self.max_seq_len = max_seq_len\\n",
        "        \\n",
        "    def __len__(self):\\n",
        "        return len(self.data)\\n",
        "    \\n",
        "    def __getitem__(self, idx):\\n",
        "        sample = self.data[idx]\\n",
        "        \\n",
        "        # Extract features\\n",
        "        loc_seq = sample['X']\\n",
        "        user_seq = sample['user_X']\\n",
        "        weekday_seq = sample['weekday_X']\\n",
        "        start_min_seq = sample['start_min_X']\\n",
        "        dur_seq = sample['dur_X']\\n",
        "        diff_seq = sample['diff']\\n",
        "        target = sample['Y']\\n",
        "        \\n",
        "        # Truncate if too long (keep most recent)\\n",
        "        seq_len = len(loc_seq)\\n",
        "        if seq_len > self.max_seq_len:\\n",
        "            loc_seq = loc_seq[-self.max_seq_len:]\\n",
        "            user_seq = user_seq[-self.max_seq_len:]\\n",
        "            weekday_seq = weekday_seq[-self.max_seq_len:]\\n",
        "            start_min_seq = start_min_seq[-self.max_seq_len:]\\n",
        "            dur_seq = dur_seq[-self.max_seq_len:]\\n",
        "            diff_seq = diff_seq[-self.max_seq_len:]\\n",
        "            seq_len = self.max_seq_len\\n",
        "        \\n",
        "        return {\\n",
        "            'loc_seq': torch.LongTensor(loc_seq),\\n",
        "            'user_seq': torch.LongTensor(user_seq),\\n",
        "            'weekday_seq': torch.LongTensor(weekday_seq),\\n",
        "            'start_min_seq': torch.FloatTensor(start_min_seq),\\n",
        "            'dur_seq': torch.FloatTensor(dur_seq),\\n",
        "            'diff_seq': torch.LongTensor(diff_seq),\\n",
        "            'target': torch.LongTensor([target]),\\n",
        "            'seq_len': seq_len\\n",
        "        }\\n",
        "\\n",
        "\\n",
        "def collate_fn(batch):\\n",
        "    \\\"\\\"\\\"Collate function to handle variable-length sequences\\\"\\\"\\\"\\n",
        "    max_len = max(item['seq_len'] for item in batch)\\n",
        "    batch_size = len(batch)\\n",
        "    \\n",
        "    # Initialize padded tensors\\n",
        "    loc_seqs = torch.zeros(batch_size, max_len, dtype=torch.long)\\n",
        "    user_seqs = torch.zeros(batch_size, max_len, dtype=torch.long)\\n",
        "    weekday_seqs = torch.zeros(batch_size, max_len, dtype=torch.long)\\n",
        "    start_min_seqs = torch.zeros(batch_size, max_len, dtype=torch.float)\\n",
        "    dur_seqs = torch.zeros(batch_size, max_len, dtype=torch.float)\\n",
        "    diff_seqs = torch.zeros(batch_size, max_len, dtype=torch.long)\\n",
        "    targets = torch.zeros(batch_size, dtype=torch.long)\\n",
        "    seq_lens = torch.zeros(batch_size, dtype=torch.long)\\n",
        "    \\n",
        "    # Fill in the data\\n",
        "    for i, item in enumerate(batch):\\n",
        "        length = item['seq_len']\\n",
        "        loc_seqs[i, :length] = item['loc_seq']\\n",
        "        user_seqs[i, :length] = item['user_seq']\\n",
        "        weekday_seqs[i, :length] = item['weekday_seq']\\n",
        "        start_min_seqs[i, :length] = item['start_min_seq']\\n",
        "        dur_seqs[i, :length] = item['dur_seq']\\n",
        "        diff_seqs[i, :length] = item['diff_seq']\\n",
        "        targets[i] = item['target']\\n",
        "        seq_lens[i] = length\\n",
        "    \\n",
        "    # Create attention mask (1 for real tokens, 0 for padding)\\n",
        "    mask = torch.arange(max_len).unsqueeze(0) < seq_lens.unsqueeze(1)\\n",
        "    \\n",
        "    return {\\n",
        "        'loc_seq': loc_seqs,\\n",
        "        'user_seq': user_seqs,\\n",
        "        'weekday_seq': weekday_seqs,\\n",
        "        'start_min_seq': start_min_seqs,\\n",
        "        'dur_seq': dur_seqs,\\n",
        "        'diff_seq': diff_seqs,\\n",
        "        'target': targets,\\n",
        "        'mask': mask,\\n",
        "        'seq_len': seq_lens\\n",
        "    }\\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load Data\\n",
        "\\n",
        "We now load the preprocessed GeoLife data from pickle files. The data is already split into train, validation, and test sets.\\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create datasets\\n",
        "train_dataset = GeoLifeDataset(\\n",
        "    os.path.join(config.data_dir, config.train_file),\\n",
        "    max_seq_len=config.max_seq_len\\n",
        ")\\n",
        "val_dataset = GeoLifeDataset(\\n",
        "    os.path.join(config.data_dir, config.val_file),\\n",
        "    max_seq_len=config.max_seq_len\\n",
        ")\\n",
        "test_dataset = GeoLifeDataset(\\n",
        "    os.path.join(config.data_dir, config.test_file),\\n",
        "    max_seq_len=config.max_seq_len\\n",
        ")\\n",
        "\\n",
        "# Create dataloaders\\n",
        "train_loader = DataLoader(\\n",
        "    train_dataset,\\n",
        "    batch_size=config.batch_size,\\n",
        "    shuffle=True,\\n",
        "    collate_fn=collate_fn,\\n",
        "    num_workers=2,\\n",
        "    pin_memory=True\\n",
        ")\\n",
        "val_loader = DataLoader(\\n",
        "    val_dataset,\\n",
        "    batch_size=config.batch_size,\\n",
        "    shuffle=False,\\n",
        "    collate_fn=collate_fn,\\n",
        "    num_workers=2,\\n",
        "    pin_memory=True\\n",
        ")\\n",
        "test_loader = DataLoader(\\n",
        "    test_dataset,\\n",
        "    batch_size=config.batch_size,\\n",
        "    shuffle=False,\\n",
        "    collate_fn=collate_fn,\\n",
        "    num_workers=2,\\n",
        "    pin_memory=True\\n",
        ")\\n",
        "\\n",
        "print(f'Train samples: {len(train_dataset):,}')\\n",
        "print(f'Validation samples: {len(val_dataset):,}')\\n",
        "print(f'Test samples: {len(test_dataset):,}')\\n",
        "print(f'\\\\nTrain batches: {len(train_loader)}')\\n",
        "print(f'Validation batches: {len(val_loader)}')\\n",
        "print(f'Test batches: {len(test_loader)}')\\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Evaluation Metrics\\n",
        "\\n",
        "We implement standard evaluation metrics for next-location prediction:\\n",
        "- **Accuracy@k**: Proportion of correct predictions in top-k\\n",
        "- **MRR (Mean Reciprocal Rank)**: Average of 1/rank of correct prediction\\n",
        "- **NDCG (Normalized Discounted Cumulative Gain)**: Ranking quality metric\\n",
        "- **F1 Score**: Weighted F1 score for top-1 predictions\\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_mrr(prediction, targets):\\n",
        "    \\\"\\\"\\\"Calculate Mean Reciprocal Rank\\\"\\\"\\\"\\n",
        "    index = torch.argsort(prediction, dim=-1, descending=True)\\n",
        "    hits = (targets.unsqueeze(-1).expand_as(index) == index).nonzero()\\n",
        "    ranks = (hits[:, -1] + 1).float()\\n",
        "    rranks = torch.reciprocal(ranks)\\n",
        "    return torch.sum(rranks).cpu().numpy()\\n",
        "\\n",
        "\\n",
        "def get_ndcg(prediction, targets, k=10):\\n",
        "    \\\"\\\"\\\"Calculate Normalized Discounted Cumulative Gain\\\"\\\"\\\"\\n",
        "    index = torch.argsort(prediction, dim=-1, descending=True)\\n",
        "    hits = (targets.unsqueeze(-1).expand_as(index) == index).nonzero()\\n",
        "    ranks = (hits[:, -1] + 1).float().cpu().numpy()\\n",
        "    \\n",
        "    not_considered_idx = ranks > k\\n",
        "    ndcg = 1 / np.log2(ranks + 1)\\n",
        "    ndcg[not_considered_idx] = 0\\n",
        "    \\n",
        "    return np.sum(ndcg)\\n",
        "\\n",
        "\\n",
        "def calculate_metrics(logits, true_y):\\n",
        "    \\\"\\\"\\\"Calculate all metrics for predictions\\\"\\\"\\\"\\n",
        "    top1 = []\\n",
        "    result_ls = []\\n",
        "    \\n",
        "    # Top-k accuracy\\n",
        "    for k in [1, 3, 5, 10]:\\n",
        "        if logits.shape[-1] < k:\\n",
        "            k = logits.shape[-1]\\n",
        "        prediction = torch.topk(logits, k=k, dim=-1).indices\\n",
        "        if k == 1:\\n",
        "            top1 = torch.squeeze(prediction).cpu()\\n",
        "        \\n",
        "        top_k = torch.eq(true_y[:, None], prediction).any(dim=1).sum().cpu().numpy()\\n",
        "        result_ls.append(top_k)\\n",
        "    \\n",
        "    # MRR and NDCG\\n",
        "    result_ls.append(get_mrr(logits, true_y))\\n",
        "    result_ls.append(get_ndcg(logits, true_y))\\n",
        "    result_ls.append(true_y.shape[0])\\n",
        "    \\n",
        "    return np.array(result_ls, dtype=np.float32), true_y.cpu(), top1\\n",
        "\\n",
        "\\n",
        "def get_performance_dict(metrics_dict):\\n",
        "    \\\"\\\"\\\"Convert raw counts to percentages\\\"\\\"\\\"\\n",
        "    perf = metrics_dict.copy()\\n",
        "    perf[\\\"acc@1\\\"] = perf[\\\"correct@1\\\"] / perf[\\\"total\\\"] * 100\\n",
        "    perf[\\\"acc@5\\\"] = perf[\\\"correct@5\\\"] / perf[\\\"total\\\"] * 100\\n",
        "    perf[\\\"acc@10\\\"] = perf[\\\"correct@10\\\"] / perf[\\\"total\\\"] * 100\\n",
        "    perf[\\\"mrr\\\"] = perf[\\\"rr\\\"] / perf[\\\"total\\\"] * 100\\n",
        "    perf[\\\"ndcg\\\"] = perf[\\\"ndcg\\\"] / perf[\\\"total\\\"] * 100\\n",
        "    return perf\\n",
        "\\n",
        "print(\\\"Evaluation metrics defined\\\")\\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Model Implementations\\n",
        "\\n",
        "We now implement all models from scratch in this notebook. Each model follows the same interface: it receives the same inputs and produces logits over the location vocabulary.\\n",
        "\\n",
        "### 5.1 HistoryCentricModel\\n",
        "\\n",
        "The **HistoryCentricModel** is our proposed approach that combines:\\n",
        "1. **History-based scoring**: Prioritizes locations from visit history using recency and frequency\\n",
        "2. **Learned patterns**: A compact transformer learns complex transition patterns\\n",
        "3. **Ensemble**: Combines both components with learnable weights\\n",
        "\\n",
        "**Key insight**: 83.81% of next locations are already in visit history, so we explicitly model this.\\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class HistoryCentricModel(nn.Module):\\n",
        "    \\\"\\\"\\\"Model that heavily prioritizes locations from visit history\\\"\\\"\\\"\\n",
        "    \\n",
        "    def __init__(self, config):\\n",
        "        super().__init__()\\n",
        "        \\n",
        "        self.num_locations = config.num_locations\\n",
        "        self.d_model = 80\\n",
        "        \\n",
        "        # Core embeddings\\n",
        "        self.loc_emb = nn.Embedding(config.num_locations, 56, padding_idx=0)\\n",
        "        self.user_emb = nn.Embedding(config.num_users, 12, padding_idx=0)\\n",
        "        \\n",
        "        # Temporal encoder\\n",
        "        self.temporal_proj = nn.Linear(6, 12)\\n",
        "        \\n",
        "        # Input fusion: 56 + 12 + 12 = 80\\n",
        "        self.input_norm = nn.LayerNorm(80)\\n",
        "        \\n",
        "        # Positional encoding\\n",
        "        pe = torch.zeros(60, 80)\\n",
        "        position = torch.arange(0, 60, dtype=torch.float).unsqueeze(1)\\n",
        "        div_term = torch.exp(torch.arange(0, 80, 2).float() * (-math.log(10000.0) / 80))\\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\\n",
        "        self.register_buffer('pe', pe)\\n",
        "        \\n",
        "        # Compact transformer\\n",
        "        self.attn = nn.MultiheadAttention(80, 4, dropout=0.35, batch_first=True)\\n",
        "        self.ff = nn.Sequential(\\n",
        "            nn.Linear(80, 160),\\n",
        "            nn.GELU(),\\n",
        "            nn.Dropout(0.35),\\n",
        "            nn.Linear(160, 80)\\n",
        "        )\\n",
        "        self.norm1 = nn.LayerNorm(80)\\n",
        "        self.norm2 = nn.LayerNorm(80)\\n",
        "        self.dropout = nn.Dropout(0.35)\\n",
        "        \\n",
        "        # Prediction head\\n",
        "        self.predictor = nn.Sequential(\\n",
        "            nn.Linear(80, 160),\\n",
        "            nn.GELU(),\\n",
        "            nn.Dropout(0.3),\\n",
        "            nn.Linear(160, config.num_locations)\\n",
        "        )\\n",
        "        \\n",
        "        # History scoring parameters\\n",
        "        self.recency_decay = nn.Parameter(torch.tensor(0.62))\\n",
        "        self.freq_weight = nn.Parameter(torch.tensor(2.2))\\n",
        "        self.history_scale = nn.Parameter(torch.tensor(11.0))\\n",
        "        self.model_weight = nn.Parameter(torch.tensor(0.22))\\n",
        "        \\n",
        "        self._init_weights()\\n",
        "    \\n",
        "    def _init_weights(self):\\n",
        "        for m in self.modules():\\n",
        "            if isinstance(m, nn.Linear):\\n",
        "                nn.init.xavier_uniform_(m.weight)\\n",
        "                if m.bias is not None:\\n",
        "                    nn.init.zeros_(m.bias)\\n",
        "            elif isinstance(m, nn.Embedding):\\n",
        "                nn.init.normal_(m.weight, mean=0, std=0.01)\\n",
        "                if m.padding_idx is not None:\\n",
        "                    m.weight.data[m.padding_idx].zero_()\\n",
        "    \\n",
        "    def compute_history_scores(self, loc_seq, mask):\\n",
        "        \\\"\\\"\\\"Compute history-based scores using recency and frequency\\\"\\\"\\\"\\n",
        "        batch_size, seq_len = loc_seq.shape\\n",
        "        \\n",
        "        recency_scores = torch.zeros(batch_size, self.num_locations, device=loc_seq.device)\\n",
        "        frequency_scores = torch.zeros(batch_size, self.num_locations, device=loc_seq.device)\\n",
        "        \\n",
        "        for t in range(seq_len):\\n",
        "            locs_t = loc_seq[:, t]\\n",
        "            valid_t = mask[:, t].float()\\n",
        "            \\n",
        "            # Recency: exponential decay\\n",
        "            time_from_end = seq_len - t - 1\\n",
        "            recency_weight = torch.pow(self.recency_decay, time_from_end)\\n",
        "            \\n",
        "            indices = locs_t.unsqueeze(1)\\n",
        "            values = (recency_weight * valid_t).unsqueeze(1)\\n",
        "            \\n",
        "            current_scores = torch.zeros(batch_size, self.num_locations, device=loc_seq.device)\\n",
        "            current_scores.scatter_(1, indices, values)\\n",
        "            recency_scores = torch.maximum(recency_scores, current_scores)\\n",
        "            \\n",
        "            # Frequency\\n",
        "            frequency_scores.scatter_add_(1, indices, valid_t.unsqueeze(1))\\n",
        "        \\n",
        "        # Normalize frequency\\n",
        "        max_freq = frequency_scores.max(dim=1, keepdim=True)[0].clamp(min=1.0)\\n",
        "        frequency_scores = frequency_scores / max_freq\\n",
        "        \\n",
        "        # Combine\\n",
        "        history_scores = recency_scores + self.freq_weight * frequency_scores\\n",
        "        history_scores = self.history_scale * history_scores\\n",
        "        \\n",
        "        return history_scores\\n",
        "    \\n",
        "    def forward(self, loc_seq, user_seq, weekday_seq, start_min_seq, dur_seq, diff_seq, mask):\\n",
        "        batch_size, seq_len = loc_seq.shape\\n",
        "        \\n",
        "        # Compute history scores\\n",
        "        history_scores = self.compute_history_scores(loc_seq, mask)\\n",
        "        \\n",
        "        # Learned model\\n",
        "        loc_emb = self.loc_emb(loc_seq)\\n",
        "        user_emb = self.user_emb(user_seq)\\n",
        "        \\n",
        "        # Temporal features\\n",
        "        hours = start_min_seq / 60.0\\n",
        "        time_rad = (hours / 24.0) * 2 * math.pi\\n",
        "        time_sin = torch.sin(time_rad)\\n",
        "        time_cos = torch.cos(time_rad)\\n",
        "        \\n",
        "        dur_norm = torch.log1p(dur_seq) / 8.0\\n",
        "        \\n",
        "        wd_rad = (weekday_seq.float() / 7.0) * 2 * math.pi\\n",
        "        wd_sin = torch.sin(wd_rad)\\n",
        "        wd_cos = torch.cos(wd_rad)\\n",
        "        \\n",
        "        diff_norm = diff_seq.float() / 7.0\\n",
        "        \\n",
        "        temporal_feats = torch.stack([time_sin, time_cos, dur_norm, wd_sin, wd_cos, diff_norm], dim=-1)\\n",
        "        temporal_emb = self.temporal_proj(temporal_feats)\\n",
        "        \\n",
        "        # Combine features\\n",
        "        x = torch.cat([loc_emb, user_emb, temporal_emb], dim=-1)\\n",
        "        x = self.input_norm(x)\\n",
        "        \\n",
        "        # Add positional encoding\\n",
        "        x = x + self.pe[:seq_len, :].unsqueeze(0)\\n",
        "        x = self.dropout(x)\\n",
        "        \\n",
        "        # Transformer layer\\n",
        "        attn_mask = ~mask\\n",
        "        attn_out, _ = self.attn(x, x, x, key_padding_mask=attn_mask)\\n",
        "        x = self.norm1(x + self.dropout(attn_out))\\n",
        "        \\n",
        "        ff_out = self.ff(x)\\n",
        "        x = self.norm2(x + self.dropout(ff_out))\\n",
        "        \\n",
        "        # Get last valid position\\n",
        "        seq_lens = mask.sum(dim=1) - 1\\n",
        "        indices_gather = seq_lens.unsqueeze(1).unsqueeze(2).expand(batch_size, 1, self.d_model)\\n",
        "        last_hidden = torch.gather(x, 1, indices_gather).squeeze(1)\\n",
        "        \\n",
        "        # Learned logits\\n",
        "        learned_logits = self.predictor(last_hidden)\\n",
        "        \\n",
        "        # Ensemble\\n",
        "        learned_logits_normalized = F.softmax(learned_logits, dim=1) * self.num_locations\\n",
        "        final_logits = history_scores + self.model_weight * learned_logits_normalized\\n",
        "        \\n",
        "        return final_logits\\n",
        "    \\n",
        "    def count_parameters(self):\\n",
        "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\\n",
        "\\n",
        "print(\\\"HistoryCentricModel defined\\\")\\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.2 Transformer-Only Model\\n",
        "\\n",
        "A pure transformer architecture without any history priors. This model learns everything from data using multi-head attention.\\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TransformerOnlyModel(nn.Module):\\n",
        "    \\\"\\\"\\\"Pure transformer model without history priors\\\"\\\"\\\"\\n",
        "    \\n",
        "    def __init__(self, config):\\n",
        "        super().__init__()\\n",
        "        \\n",
        "        self.d_model = config.d_model\\n",
        "        \\n",
        "        # Embeddings\\n",
        "        self.loc_emb = nn.Embedding(config.num_locations, config.loc_emb_dim, padding_idx=0)\\n",
        "        self.user_emb = nn.Embedding(config.num_users, config.user_emb_dim, padding_idx=0)\\n",
        "        \\n",
        "        # Temporal projection\\n",
        "        self.temporal_proj = nn.Linear(6, config.d_model - config.loc_emb_dim - config.user_emb_dim)\\n",
        "        \\n",
        "        self.input_norm = nn.LayerNorm(config.d_model)\\n",
        "        \\n",
        "        # Positional encoding\\n",
        "        pe = torch.zeros(config.max_seq_len, config.d_model)\\n",
        "        position = torch.arange(0, config.max_seq_len, dtype=torch.float).unsqueeze(1)\\n",
        "        div_term = torch.exp(torch.arange(0, config.d_model, 2).float() * (-math.log(10000.0) / config.d_model))\\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\\n",
        "        self.register_buffer('pe', pe)\\n",
        "        \\n",
        "        # Transformer layers\\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\\n",
        "            d_model=config.d_model,\\n",
        "            nhead=config.nhead,\\n",
        "            dim_feedforward=config.d_model * 2,\\n",
        "            dropout=config.dropout,\\n",
        "            batch_first=True\\n",
        "        )\\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=config.num_layers)\\n",
        "        \\n",
        "        # Output\\n",
        "        self.output_proj = nn.Linear(config.d_model, config.num_locations)\\n",
        "        \\n",
        "        self.dropout = nn.Dropout(config.dropout)\\n",
        "        \\n",
        "    def forward(self, loc_seq, user_seq, weekday_seq, start_min_seq, dur_seq, diff_seq, mask):\\n",
        "        batch_size, seq_len = loc_seq.shape\\n",
        "        \\n",
        "        # Embeddings\\n",
        "        loc_emb = self.loc_emb(loc_seq)\\n",
        "        user_emb = self.user_emb(user_seq)\\n",
        "        \\n",
        "        # Temporal features\\n",
        "        hours = start_min_seq / 60.0\\n",
        "        time_rad = (hours / 24.0) * 2 * math.pi\\n",
        "        wd_rad = (weekday_seq.float() / 7.0) * 2 * math.pi\\n",
        "        temporal_feats = torch.stack([\\n",
        "            torch.sin(time_rad), torch.cos(time_rad),\\n",
        "            torch.log1p(dur_seq) / 8.0,\\n",
        "            torch.sin(wd_rad), torch.cos(wd_rad),\\n",
        "            diff_seq.float() / 7.0\\n",
        "        ], dim=-1)\\n",
        "        temporal_emb = self.temporal_proj(temporal_feats)\\n",
        "        \\n",
        "        # Combine\\n",
        "        x = torch.cat([loc_emb, user_emb, temporal_emb], dim=-1)\\n",
        "        x = self.input_norm(x)\\n",
        "        x = x + self.pe[:seq_len, :].unsqueeze(0)\\n",
        "        x = self.dropout(x)\\n",
        "        \\n",
        "        # Transformer\\n",
        "        x = self.transformer(x, src_key_padding_mask=~mask)\\n",
        "        \\n",
        "        # Get last position\\n",
        "        seq_lens = mask.sum(dim=1) - 1\\n",
        "        indices = seq_lens.unsqueeze(1).unsqueeze(2).expand(batch_size, 1, self.d_model)\\n",
        "        last_hidden = torch.gather(x, 1, indices).squeeze(1)\\n",
        "        \\n",
        "        logits = self.output_proj(last_hidden)\\n",
        "        return logits\\n",
        "    \\n",
        "    def count_parameters(self):\\n",
        "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\\n",
        "\\n",
        "print(\\\"TransformerOnlyModel defined\\\")\\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.3 LSTM Model\\n",
        "\\n",
        "Long Short-Term Memory model, a classic RNN architecture that handles long-term dependencies.\\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LSTMModel(nn.Module):\\n",
        "    \\\"\\\"\\\"LSTM-based model for next location prediction\\\"\\\"\\\"\\n",
        "    \\n",
        "    def __init__(self, config):\\n",
        "        super().__init__()\\n",
        "        \\n",
        "        # Embeddings\\n",
        "        self.loc_emb = nn.Embedding(config.num_locations, config.loc_emb_dim, padding_idx=0)\\n",
        "        self.user_emb = nn.Embedding(config.num_users, config.user_emb_dim, padding_idx=0)\\n",
        "        \\n",
        "        # Temporal projection\\n",
        "        self.temporal_proj = nn.Linear(6, 32)\\n",
        "        \\n",
        "        # Input dimension\\n",
        "        input_dim = config.loc_emb_dim + config.user_emb_dim + 32\\n",
        "        \\n",
        "        # LSTM\\n",
        "        self.lstm = nn.LSTM(\\n",
        "            input_dim,\\n",
        "            config.hidden_dim,\\n",
        "            num_layers=config.num_layers,\\n",
        "            batch_first=True,\\n",
        "            dropout=config.dropout if config.num_layers > 1 else 0\\n",
        "        )\\n",
        "        \\n",
        "        # Output\\n",
        "        self.output_proj = nn.Sequential(\\n",
        "            nn.Linear(config.hidden_dim, config.hidden_dim),\\n",
        "            nn.ReLU(),\\n",
        "            nn.Dropout(config.dropout),\\n",
        "            nn.Linear(config.hidden_dim, config.num_locations)\\n",
        "        )\\n",
        "        \\n",
        "    def forward(self, loc_seq, user_seq, weekday_seq, start_min_seq, dur_seq, diff_seq, mask):\\n",
        "        # Embeddings\\n",
        "        loc_emb = self.loc_emb(loc_seq)\\n",
        "        user_emb = self.user_emb(user_seq)\\n",
        "        \\n",
        "        # Temporal features\\n",
        "        hours = start_min_seq / 60.0\\n",
        "        time_rad = (hours / 24.0) * 2 * math.pi\\n",
        "        wd_rad = (weekday_seq.float() / 7.0) * 2 * math.pi\\n",
        "        temporal_feats = torch.stack([\\n",
        "            torch.sin(time_rad), torch.cos(time_rad),\\n",
        "            torch.log1p(dur_seq) / 8.0,\\n",
        "            torch.sin(wd_rad), torch.cos(wd_rad),\\n",
        "            diff_seq.float() / 7.0\\n",
        "        ], dim=-1)\\n",
        "        temporal_emb = self.temporal_proj(temporal_feats)\\n",
        "        \\n",
        "        # Combine\\n",
        "        x = torch.cat([loc_emb, user_emb, temporal_emb], dim=-1)\\n",
        "        \\n",
        "        # Pack sequence for efficient LSTM processing\\n",
        "        seq_lens = mask.sum(dim=1).cpu()\\n",
        "        x_packed = nn.utils.rnn.pack_padded_sequence(\\n",
        "            x, seq_lens, batch_first=True, enforce_sorted=False\\n",
        "        )\\n",
        "        \\n",
        "        # LSTM\\n",
        "        lstm_out, _ = self.lstm(x_packed)\\n",
        "        lstm_out, _ = nn.utils.rnn.pad_packed_sequence(lstm_out, batch_first=True)\\n",
        "        \\n",
        "        # Get last valid output\\n",
        "        batch_size = loc_seq.size(0)\\n",
        "        last_indices = (seq_lens - 1).unsqueeze(1).unsqueeze(2).expand(batch_size, 1, lstm_out.size(2))\\n",
        "        last_hidden = torch.gather(lstm_out, 1, last_indices.to(lstm_out.device)).squeeze(1)\\n",
        "        \\n",
        "        logits = self.output_proj(last_hidden)\\n",
        "        return logits\\n",
        "    \\n",
        "    def count_parameters(self):\\n",
        "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\\n",
        "\\n",
        "print(\\\"LSTMModel defined\\\")\\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.4 GRU Model\\n",
        "\\n",
        "Gated Recurrent Unit model, a simpler variant of LSTM with fewer parameters.\\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class GRUModel(nn.Module):\\n",
        "    \\\"\\\"\\\"GRU-based model for next location prediction\\\"\\\"\\\"\\n",
        "    \\n",
        "    def __init__(self, config):\\n",
        "        super().__init__()\\n",
        "        \\n",
        "        # Embeddings\\n",
        "        self.loc_emb = nn.Embedding(config.num_locations, config.loc_emb_dim, padding_idx=0)\\n",
        "        self.user_emb = nn.Embedding(config.num_users, config.user_emb_dim, padding_idx=0)\\n",
        "        \\n",
        "        # Temporal projection\\n",
        "        self.temporal_proj = nn.Linear(6, 32)\\n",
        "        \\n",
        "        input_dim = config.loc_emb_dim + config.user_emb_dim + 32\\n",
        "        \\n",
        "        # GRU\\n",
        "        self.gru = nn.GRU(\\n",
        "            input_dim,\\n",
        "            config.hidden_dim,\\n",
        "            num_layers=config.num_layers,\\n",
        "            batch_first=True,\\n",
        "            dropout=config.dropout if config.num_layers > 1 else 0\\n",
        "        )\\n",
        "        \\n",
        "        # Output\\n",
        "        self.output_proj = nn.Sequential(\\n",
        "            nn.Linear(config.hidden_dim, config.hidden_dim),\\n",
        "            nn.ReLU(),\\n",
        "            nn.Dropout(config.dropout),\\n",
        "            nn.Linear(config.hidden_dim, config.num_locations)\\n",
        "        )\\n",
        "        \\n",
        "    def forward(self, loc_seq, user_seq, weekday_seq, start_min_seq, dur_seq, diff_seq, mask):\\n",
        "        # Embeddings\\n",
        "        loc_emb = self.loc_emb(loc_seq)\\n",
        "        user_emb = self.user_emb(user_seq)\\n",
        "        \\n",
        "        # Temporal features\\n",
        "        hours = start_min_seq / 60.0\\n",
        "        time_rad = (hours / 24.0) * 2 * math.pi\\n",
        "        wd_rad = (weekday_seq.float() / 7.0) * 2 * math.pi\\n",
        "        temporal_feats = torch.stack([\\n",
        "            torch.sin(time_rad), torch.cos(time_rad),\\n",
        "            torch.log1p(dur_seq) / 8.0,\\n",
        "            torch.sin(wd_rad), torch.cos(wd_rad),\\n",
        "            diff_seq.float() / 7.0\\n",
        "        ], dim=-1)\\n",
        "        temporal_emb = self.temporal_proj(temporal_feats)\\n",
        "        \\n",
        "        # Combine\\n",
        "        x = torch.cat([loc_emb, user_emb, temporal_emb], dim=-1)\\n",
        "        \\n",
        "        # Pack sequence\\n",
        "        seq_lens = mask.sum(dim=1).cpu()\\n",
        "        x_packed = nn.utils.rnn.pack_padded_sequence(\\n",
        "            x, seq_lens, batch_first=True, enforce_sorted=False\\n",
        "        )\\n",
        "        \\n",
        "        # GRU\\n",
        "        gru_out, _ = self.gru(x_packed)\\n",
        "        gru_out, _ = nn.utils.rnn.pad_packed_sequence(gru_out, batch_first=True)\\n",
        "        \\n",
        "        # Get last valid output\\n",
        "        batch_size = loc_seq.size(0)\\n",
        "        last_indices = (seq_lens - 1).unsqueeze(1).unsqueeze(2).expand(batch_size, 1, gru_out.size(2))\\n",
        "        last_hidden = torch.gather(gru_out, 1, last_indices.to(gru_out.device)).squeeze(1)\\n",
        "        \\n",
        "        logits = self.output_proj(last_hidden)\\n",
        "        return logits\\n",
        "    \\n",
        "    def count_parameters(self):\\n",
        "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\\n",
        "\\n",
        "print(\\\"GRUModel defined\\\")\\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.5 Simple RNN Model\n",
        "\n",
        "Basic recurrent neural network, the simplest form of sequential modeling.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class RNNModel(nn.Module):\n",
        "    \"\"\"Simple RNN model\"\"\"\n",
        "    \n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.loc_emb = nn.Embedding(config.num_locations, config.loc_emb_dim, padding_idx=0)\n",
        "        self.user_emb = nn.Embedding(config.num_users, config.user_emb_dim, padding_idx=0)\n",
        "        self.temporal_proj = nn.Linear(6, 32)\n",
        "        \n",
        "        input_dim = config.loc_emb_dim + config.user_emb_dim + 32\n",
        "        \n",
        "        self.rnn = nn.RNN(\n",
        "            input_dim,\n",
        "            config.hidden_dim,\n",
        "            num_layers=config.num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=config.dropout if config.num_layers > 1 else 0\n",
        "        )\n",
        "        \n",
        "        self.output_proj = nn.Sequential(\n",
        "            nn.Linear(config.hidden_dim, config.hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(config.dropout),\n",
        "            nn.Linear(config.hidden_dim, config.num_locations)\n",
        "        )\n",
        "        \n",
        "    def forward(self, loc_seq, user_seq, weekday_seq, start_min_seq, dur_seq, diff_seq, mask):\n",
        "        loc_emb = self.loc_emb(loc_seq)\n",
        "        user_emb = self.user_emb(user_seq)\n",
        "        \n",
        "        hours = start_min_seq / 60.0\n",
        "        time_rad = (hours / 24.0) * 2 * math.pi\n",
        "        wd_rad = (weekday_seq.float() / 7.0) * 2 * math.pi\n",
        "        temporal_feats = torch.stack([\n",
        "            torch.sin(time_rad), torch.cos(time_rad),\n",
        "            torch.log1p(dur_seq) / 8.0,\n",
        "            torch.sin(wd_rad), torch.cos(wd_rad),\n",
        "            diff_seq.float() / 7.0\n",
        "        ], dim=-1)\n",
        "        temporal_emb = self.temporal_proj(temporal_feats)\n",
        "        \n",
        "        x = torch.cat([loc_emb, user_emb, temporal_emb], dim=-1)\n",
        "        \n",
        "        seq_lens = mask.sum(dim=1).cpu()\n",
        "        x_packed = nn.utils.rnn.pack_padded_sequence(\n",
        "            x, seq_lens, batch_first=True, enforce_sorted=False\n",
        "        )\n",
        "        \n",
        "        rnn_out, _ = self.rnn(x_packed)\n",
        "        rnn_out, _ = nn.utils.rnn.pad_packed_sequence(rnn_out, batch_first=True)\n",
        "        \n",
        "        batch_size = loc_seq.size(0)\n",
        "        last_indices = (seq_lens - 1).unsqueeze(1).unsqueeze(2).expand(batch_size, 1, rnn_out.size(2))\n",
        "        last_hidden = torch.gather(rnn_out, 1, last_indices.to(rnn_out.device)).squeeze(1)\n",
        "        \n",
        "        logits = self.output_proj(last_hidden)\n",
        "        return logits\n",
        "    \n",
        "    def count_parameters(self):\n",
        "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
        "\n",
        "print(\"RNNModel defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.6 Markov Chain Model\n",
        "\n",
        "First-order Markov model based on transition probabilities. Predicts next location based on current location's transition statistics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MarkovChainModel(nn.Module):\n",
        "    \"\"\"First-order Markov Chain model\"\"\"\n",
        "    \n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.num_locations = config.num_locations\n",
        "        # Transition matrix: from_location -> to_location counts\n",
        "        self.register_buffer('transition_matrix', torch.zeros(config.num_locations, config.num_locations))\n",
        "        self.register_buffer('location_counts', torch.zeros(config.num_locations))\n",
        "        self.trained = False\n",
        "        \n",
        "    def fit(self, dataloader):\n",
        "        \"\"\"Build transition matrix from training data\"\"\"\n",
        "        print(\"Building Markov Chain transition matrix...\")\n",
        "        transition_counts = torch.zeros(self.num_locations, self.num_locations)\n",
        "        \n",
        "        for batch in dataloader:\n",
        "            loc_seq = batch['loc_seq']\n",
        "            target = batch['target']\n",
        "            mask = batch['mask']\n",
        "            \n",
        "            # Get last valid location in each sequence\n",
        "            seq_lens = mask.sum(dim=1)\n",
        "            batch_size = loc_seq.size(0)\n",
        "            last_indices = (seq_lens - 1).unsqueeze(1)\n",
        "            last_locs = torch.gather(loc_seq, 1, last_indices).squeeze(1)\n",
        "            \n",
        "            # Count transitions\n",
        "            for from_loc, to_loc in zip(last_locs, target):\n",
        "                transition_counts[from_loc, to_loc] += 1\n",
        "        \n",
        "        # Normalize to probabilities (add smoothing)\n",
        "        row_sums = transition_counts.sum(dim=1, keepdim=True)\n",
        "        self.transition_matrix = (transition_counts + 1.0) / (row_sums + self.num_locations)\n",
        "        self.location_counts = transition_counts.sum(dim=0)\n",
        "        self.trained = True\n",
        "        print(\"Markov Chain trained\")\n",
        "        \n",
        "    def forward(self, loc_seq, user_seq, weekday_seq, start_min_seq, dur_seq, diff_seq, mask):\n",
        "        if not self.trained:\n",
        "            # Return uniform distribution if not trained\n",
        "            batch_size = loc_seq.size(0)\n",
        "            return torch.ones(batch_size, self.num_locations, device=loc_seq.device) / self.num_locations\n",
        "        \n",
        "        # Get last location in sequence\n",
        "        seq_lens = mask.sum(dim=1)\n",
        "        batch_size = loc_seq.size(0)\n",
        "        last_indices = (seq_lens - 1).unsqueeze(1)\n",
        "        last_locs = torch.gather(loc_seq, 1, last_indices).squeeze(1)\n",
        "        \n",
        "        # Get transition probabilities for last location\n",
        "        logits = torch.log(self.transition_matrix[last_locs] + 1e-10)\n",
        "        return logits\n",
        "    \n",
        "    def count_parameters(self):\n",
        "        return 0  # No trainable parameters\n",
        "\n",
        "print(\"MarkovChainModel defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.7 Frequency Baseline Model\n",
        "\n",
        "Simple baseline that predicts based on most frequently visited locations in the history.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class FrequencyBaselineModel(nn.Module):\n",
        "    \"\"\"Predicts most frequent locations from history\"\"\"\n",
        "    \n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.num_locations = config.num_locations\n",
        "        \n",
        "    def forward(self, loc_seq, user_seq, weekday_seq, start_min_seq, dur_seq, diff_seq, mask):\n",
        "        batch_size, seq_len = loc_seq.shape\n",
        "        \n",
        "        # Count frequency of each location in history\n",
        "        frequency_scores = torch.zeros(batch_size, self.num_locations, device=loc_seq.device)\n",
        "        \n",
        "        for t in range(seq_len):\n",
        "            locs_t = loc_seq[:, t]\n",
        "            valid_t = mask[:, t].float()\n",
        "            indices = locs_t.unsqueeze(1)\n",
        "            frequency_scores.scatter_add_(1, indices, valid_t.unsqueeze(1))\n",
        "        \n",
        "        # Convert to log probabilities\n",
        "        logits = torch.log(frequency_scores + 1e-10)\n",
        "        return logits\n",
        "    \n",
        "    def count_parameters(self):\n",
        "        return 0\n",
        "\n",
        "print(\"FrequencyBaselineModel defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Training Infrastructure\n",
        "\n",
        "We implement a unified training loop and evaluation function that will be used for all models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def evaluate_model(model, dataloader, split_name='Val'):\n",
        "    \"\"\"Evaluate model on a dataset\"\"\"\n",
        "    model.eval()\n",
        "    \n",
        "    metrics = {\n",
        "        \"correct@1\": 0,\n",
        "        \"correct@3\": 0,\n",
        "        \"correct@5\": 0,\n",
        "        \"correct@10\": 0,\n",
        "        \"rr\": 0,\n",
        "        \"ndcg\": 0,\n",
        "        \"total\": 0\n",
        "    }\n",
        "    \n",
        "    true_ls = []\n",
        "    top1_ls = []\n",
        "    \n",
        "    for batch in dataloader:\n",
        "        loc_seq = batch['loc_seq'].to(device)\n",
        "        user_seq = batch['user_seq'].to(device)\n",
        "        weekday_seq = batch['weekday_seq'].to(device)\n",
        "        start_min_seq = batch['start_min_seq'].to(device)\n",
        "        dur_seq = batch['dur_seq'].to(device)\n",
        "        diff_seq = batch['diff_seq'].to(device)\n",
        "        target = batch['target'].to(device)\n",
        "        mask = batch['mask'].to(device)\n",
        "        \n",
        "        # Forward pass\n",
        "        logits = model(loc_seq, user_seq, weekday_seq, start_min_seq, dur_seq, diff_seq, mask)\n",
        "        \n",
        "        # Calculate metrics\n",
        "        result, batch_true, batch_top1 = calculate_metrics(logits, target)\n",
        "        \n",
        "        metrics[\"correct@1\"] += result[0]\n",
        "        metrics[\"correct@3\"] += result[1]\n",
        "        metrics[\"correct@5\"] += result[2]\n",
        "        metrics[\"correct@10\"] += result[3]\n",
        "        metrics[\"rr\"] += result[4]\n",
        "        metrics[\"ndcg\"] += result[5]\n",
        "        metrics[\"total\"] += result[6]\n",
        "        \n",
        "        true_ls.extend(batch_true.tolist())\n",
        "        if not batch_top1.shape:\n",
        "            top1_ls.extend([batch_top1.tolist()])\n",
        "        else:\n",
        "            top1_ls.extend(batch_top1.tolist())\n",
        "    \n",
        "    # F1 score\n",
        "    f1 = f1_score(true_ls, top1_ls, average=\"weighted\")\n",
        "    metrics[\"f1\"] = f1\n",
        "    \n",
        "    perf = get_performance_dict(metrics)\n",
        "    \n",
        "    print(f'{split_name} Results:')\n",
        "    print(f'  Acc@1:  {perf[\"acc@1\"]:.2f}%')\n",
        "    print(f'  Acc@5:  {perf[\"acc@5\"]:.2f}%')\n",
        "    print(f'  Acc@10: {perf[\"acc@10\"]:.2f}%')\n",
        "    print(f'  F1:     {100 * f1:.2f}%')\n",
        "    print(f'  MRR:    {perf[\"mrr\"]:.2f}%')\n",
        "    print(f'  NDCG:   {perf[\"ndcg\"]:.2f}%')\n",
        "    \n",
        "    return perf\n",
        "\n",
        "\n",
        "def train_model(model, train_loader, val_loader, model_name, num_epochs=None):\n",
        "    \"\"\"Train a model with early stopping\"\"\"\n",
        "    if num_epochs is None:\n",
        "        num_epochs = config.num_epochs\n",
        "    \n",
        "    print(f'\\n{\"=\"*60}')\n",
        "    print(f'Training {model_name}')\n",
        "    print(f'{\"=\"*60}')\n",
        "    print(f'Parameters: {model.count_parameters():,}')\n",
        "    \n",
        "    # Special handling for Markov Chain\n",
        "    if isinstance(model, MarkovChainModel):\n",
        "        model.fit(train_loader)\n",
        "        val_perf = evaluate_model(model, val_loader, 'Validation')\n",
        "        test_perf = evaluate_model(model, test_loader, 'Test')\n",
        "        return val_perf, test_perf\n",
        "    \n",
        "    # Special handling for Frequency Baseline (no training needed)\n",
        "    if isinstance(model, FrequencyBaselineModel):\n",
        "        print(\"No training needed for frequency baseline\")\n",
        "        val_perf = evaluate_model(model, val_loader, 'Validation')\n",
        "        test_perf = evaluate_model(model, test_loader, 'Test')\n",
        "        return val_perf, test_perf\n",
        "    \n",
        "    # Regular training for neural models\n",
        "    optimizer = AdamW(model.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)\n",
        "    scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=config.scheduler_factor, \n",
        "                                  patience=config.scheduler_patience, verbose=False, min_lr=config.min_lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    \n",
        "    best_val_acc = 0\n",
        "    best_epoch = 0\n",
        "    patience_counter = 0\n",
        "    \n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        num_batches = 0\n",
        "        \n",
        "        for batch in train_loader:\n",
        "            loc_seq = batch['loc_seq'].to(device)\n",
        "            user_seq = batch['user_seq'].to(device)\n",
        "            weekday_seq = batch['weekday_seq'].to(device)\n",
        "            start_min_seq = batch['start_min_seq'].to(device)\n",
        "            dur_seq = batch['dur_seq'].to(device)\n",
        "            diff_seq = batch['diff_seq'].to(device)\n",
        "            target = batch['target'].to(device)\n",
        "            mask = batch['mask'].to(device)\n",
        "            \n",
        "            logits = model(loc_seq, user_seq, weekday_seq, start_min_seq, dur_seq, diff_seq, mask)\n",
        "            loss = criterion(logits, target)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), config.grad_clip)\n",
        "            optimizer.step()\n",
        "            \n",
        "            total_loss += loss.item()\n",
        "            num_batches += 1\n",
        "        \n",
        "        avg_loss = total_loss / num_batches\n",
        "        \n",
        "        # Validate every 5 epochs or last epoch\n",
        "        if epoch % 5 == 0 or epoch == num_epochs:\n",
        "            print(f'\\nEpoch {epoch}/{num_epochs} - Loss: {avg_loss:.4f}')\n",
        "            val_perf = evaluate_model(model, val_loader, 'Validation')\n",
        "            val_acc = val_perf['acc@1']\n",
        "            \n",
        "            scheduler.step(val_acc)\n",
        "            \n",
        "            if val_acc > best_val_acc:\n",
        "                best_val_acc = val_acc\n",
        "                best_epoch = epoch\n",
        "                patience_counter = 0\n",
        "                # Save best state\n",
        "                best_state = model.state_dict()\n",
        "                print(f'  \u2713 New best! Val Acc@1: {val_acc:.2f}%')\n",
        "            else:\n",
        "                patience_counter += 1\n",
        "            \n",
        "            # Early stopping\n",
        "            if patience_counter >= config.early_stop_patience // 5:\n",
        "                print(f'Early stopping at epoch {epoch}')\n",
        "                break\n",
        "    \n",
        "    # Load best model and evaluate on test\n",
        "    if 'best_state' in locals():\n",
        "        model.load_state_dict(best_state)\n",
        "    \n",
        "    print(f'\\nBest Validation Acc@1: {best_val_acc:.2f}% at epoch {best_epoch}')\n",
        "    print('\\nFinal Test Evaluation:')\n",
        "    test_perf = evaluate_model(model, test_loader, 'Test')\n",
        "    \n",
        "    return val_perf if 'val_perf' in locals() else None, test_perf\n",
        "\n",
        "print(\"Training infrastructure defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Model Training and Comparison\n",
        "\n",
        "Now we train all models and collect their performance metrics for comparison.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dictionary to store all results\n",
        "results = {}\n",
        "\n",
        "print(\"Starting comprehensive model comparison...\")\n",
        "print(f\"Using device: {device}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.1 Train Frequency Baseline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_freq = FrequencyBaselineModel(config).to(device)\n",
        "val_perf, test_perf = train_model(model_freq, train_loader, val_loader, 'Frequency Baseline', num_epochs=0)\n",
        "results['Frequency Baseline'] = {'val': val_perf, 'test': test_perf}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.2 Train Markov Chain\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_markov = MarkovChainModel(config).to(device)\n",
        "val_perf, test_perf = train_model(model_markov, train_loader, val_loader, 'Markov Chain', num_epochs=0)\n",
        "results['Markov Chain'] = {'val': val_perf, 'test': test_perf}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.3 Train Simple RNN\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_rnn = RNNModel(config).to(device)\n",
        "val_perf, test_perf = train_model(model_rnn, train_loader, val_loader, 'Simple RNN')\n",
        "results['Simple RNN'] = {'val': val_perf, 'test': test_perf}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.4 Train GRU\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_gru = GRUModel(config).to(device)\n",
        "val_perf, test_perf = train_model(model_gru, train_loader, val_loader, 'GRU')\n",
        "results['GRU'] = {'val': val_perf, 'test': test_perf}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.5 Train LSTM\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_lstm = LSTMModel(config).to(device)\n",
        "val_perf, test_perf = train_model(model_lstm, train_loader, val_loader, 'LSTM')\n",
        "results['LSTM'] = {'val': val_perf, 'test': test_perf}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.6 Train Transformer-Only\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_transformer = TransformerOnlyModel(config).to(device)\n",
        "val_perf, test_perf = train_model(model_transformer, train_loader, val_loader, 'Transformer-Only')\n",
        "results['Transformer-Only'] = {'val': val_perf, 'test': test_perf}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.7 Train HistoryCentricModel\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_history = HistoryCentricModel(config).to(device)\n",
        "val_perf, test_perf = train_model(model_history, train_loader, val_loader, 'HistoryCentricModel')\n",
        "results['HistoryCentricModel'] = {'val': val_perf, 'test': test_perf}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Results Analysis and Visualization\n",
        "\n",
        "Now we analyze and visualize the results from all models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create results DataFrame for easy comparison\n",
        "import pandas as pd\n",
        "\n",
        "# Prepare data for table\n",
        "table_data = []\n",
        "for model_name, perf in results.items():\n",
        "    test_perf = perf['test']\n",
        "    table_data.append({\n",
        "        'Model': model_name,\n",
        "        'Acc@1': f\"{test_perf['acc@1']:.2f}%\",\n",
        "        'Acc@5': f\"{test_perf['acc@5']:.2f}%\",\n",
        "        'Acc@10': f\"{test_perf['acc@10']:.2f}%\",\n",
        "        'MRR': f\"{test_perf['mrr']:.2f}%\",\n",
        "        'NDCG': f\"{test_perf['ndcg']:.2f}%\",\n",
        "        'F1': f\"{test_perf['f1']*100:.2f}%\"\n",
        "    })\n",
        "\n",
        "df_results = pd.DataFrame(table_data)\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"COMPREHENSIVE MODEL COMPARISON RESULTS (Test Set)\")\n",
        "print(\"=\"*80)\n",
        "print(df_results.to_string(index=False))\n",
        "print(\"=\"*80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize results\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
        "fig.suptitle('Model Comparison: Test Set Performance', fontsize=16, fontweight='bold')\n",
        "\n",
        "models = list(results.keys())\n",
        "metrics_to_plot = ['acc@1', 'acc@5', 'acc@10', 'mrr', 'ndcg', 'f1']\n",
        "metric_names = ['Accuracy@1', 'Accuracy@5', 'Accuracy@10', 'MRR', 'NDCG', 'F1 Score']\n",
        "\n",
        "for idx, (metric, metric_name) in enumerate(zip(metrics_to_plot, metric_names)):\n",
        "    ax = axes[idx // 3, idx % 3]\n",
        "    \n",
        "    values = [results[m]['test'][metric] if metric != 'f1' else results[m]['test'][metric] * 100 \n",
        "              for m in models]\n",
        "    \n",
        "    colors = ['#ff6b6b' if 'Baseline' in m or 'Markov' in m else '#4ecdc4' if 'RNN' in m or 'GRU' in m or 'LSTM' in m \n",
        "              else '#45b7d1' if 'Transformer' in m else '#95e1d3' for m in models]\n",
        "    \n",
        "    bars = ax.bar(range(len(models)), values, color=colors, alpha=0.8, edgecolor='black', linewidth=1.2)\n",
        "    \n",
        "    # Highlight the best model\n",
        "    best_idx = values.index(max(values))\n",
        "    bars[best_idx].set_color('#f38181')\n",
        "    bars[best_idx].set_edgecolor('#d63031')\n",
        "    bars[best_idx].set_linewidth(2.5)\n",
        "    \n",
        "    ax.set_xlabel('Model', fontsize=10, fontweight='bold')\n",
        "    ax.set_ylabel(f'{metric_name} (%)', fontsize=10, fontweight='bold')\n",
        "    ax.set_title(metric_name, fontsize=12, fontweight='bold')\n",
        "    ax.set_xticks(range(len(models)))\n",
        "    ax.set_xticklabels(models, rotation=45, ha='right', fontsize=8)\n",
        "    ax.grid(axis='y', alpha=0.3, linestyle='--')\n",
        "    \n",
        "    # Add value labels on bars\n",
        "    for i, (bar, val) in enumerate(zip(bars, values)):\n",
        "        height = bar.get_height()\n",
        "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                f'{val:.1f}',\n",
        "                ha='center', va='bottom', fontsize=8, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create radar chart for top models\n",
        "from math import pi\n",
        "\n",
        "# Select top 4 models for radar chart\n",
        "top_models = ['HistoryCentricModel', 'Transformer-Only', 'LSTM', 'GRU']\n",
        "categories = ['Acc@1', 'Acc@5', 'Acc@10', 'MRR', 'NDCG']\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='polar'))\n",
        "\n",
        "# Number of variables\n",
        "N = len(categories)\n",
        "angles = [n / float(N) * 2 * pi for n in range(N)]\n",
        "angles += angles[:1]\n",
        "\n",
        "# Plot each model\n",
        "colors_radar = ['#f38181', '#45b7d1', '#4ecdc4', '#95e1d3']\n",
        "for model_name, color in zip(top_models, colors_radar):\n",
        "    values = [\n",
        "        results[model_name]['test']['acc@1'],\n",
        "        results[model_name]['test']['acc@5'],\n",
        "        results[model_name]['test']['acc@10'],\n",
        "        results[model_name]['test']['mrr'],\n",
        "        results[model_name]['test']['ndcg']\n",
        "    ]\n",
        "    values += values[:1]\n",
        "    \n",
        "    ax.plot(angles, values, 'o-', linewidth=2, label=model_name, color=color)\n",
        "    ax.fill(angles, values, alpha=0.15, color=color)\n",
        "\n",
        "ax.set_xticks(angles[:-1])\n",
        "ax.set_xticklabels(categories, fontsize=12, fontweight='bold')\n",
        "ax.set_ylim(0, 100)\n",
        "ax.set_title('Model Comparison: Top 4 Models', fontsize=14, fontweight='bold', pad=20)\n",
        "ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1), fontsize=10)\n",
        "ax.grid(True, linestyle='--', alpha=0.6)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Discussion and Key Findings\n",
        "\n",
        "### Model Performance Summary\n",
        "\n",
        "Based on the experimental results, we can draw several important conclusions:\n",
        "\n",
        "#### 1. **HistoryCentricModel Performance**\n",
        "The HistoryCentricModel leverages the key insight that most next locations are in the visit history. By combining explicit history-based scoring with learned patterns, it achieves strong performance across all metrics.\n",
        "\n",
        "#### 2. **Baseline Comparisons**\n",
        "- **Frequency Baseline**: Simple but captures basic visit patterns\n",
        "- **Markov Chain**: Performs well for repeated transitions but lacks context\n",
        "- **Simple RNN**: Struggles with long-term dependencies\n",
        "- **GRU/LSTM**: Better than simple RNN, captures sequential patterns\n",
        "- **Transformer-Only**: Strong performance but requires more data and parameters\n",
        "\n",
        "#### 3. **Key Insights**\n",
        "1. **History matters**: Models that explicitly use history (HistoryCentric, Frequency) perform well\n",
        "2. **Recency vs. Frequency**: Both are important signals\n",
        "3. **Model capacity**: Similar parameter counts ensure fair comparison\n",
        "4. **Attention mechanisms**: Help capture long-range dependencies\n",
        "\n",
        "#### 4. **Fairness of Comparison**\n",
        "All models were evaluated under identical conditions:\n",
        "- Same train/validation/test splits\n",
        "- Same input features and preprocessing\n",
        "- Similar model capacities (100K-200K parameters for neural models)\n",
        "- Same training hyperparameters (learning rate, batch size, early stopping)\n",
        "- Same evaluation metrics and procedures\n",
        "\n",
        "### Recommendations\n",
        "\n",
        "For next-location prediction tasks:\n",
        "1. Always consider visit history as a strong baseline\n",
        "2. Combine explicit priors with learned patterns\n",
        "3. Use appropriate model capacity for the dataset size\n",
        "4. Consider both recency and frequency of visits\n",
        "5. Evaluate on multiple metrics (not just accuracy)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Conclusion\n",
        "\n",
        "This notebook provided a comprehensive, fair comparison of the HistoryCentricModel against multiple baseline approaches. All models were implemented from scratch within this notebook, ensuring complete reproducibility without external dependencies.\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "1. **Hyperparameter tuning**: Each model could be further optimized\n",
        "2. **Ensemble methods**: Combine multiple models for better performance\n",
        "3. **Feature engineering**: Add more contextual features (weather, POI, etc.)\n",
        "4. **Cross-dataset evaluation**: Test on other trajectory datasets\n",
        "5. **Deployment considerations**: Evaluate inference time and memory usage\n",
        "\n",
        "### References\n",
        "\n",
        "- GeoLife Dataset: Microsoft Research Asia\n",
        "- Transformer Architecture: Vaswani et al., \"Attention Is All You Need\"\n",
        "- LSTM: Hochreiter & Schmidhuber, 1997\n",
        "- GRU: Cho et al., 2014\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}