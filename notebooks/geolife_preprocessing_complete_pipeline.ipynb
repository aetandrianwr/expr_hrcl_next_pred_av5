{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Geolife Dataset: Complete Preprocessing Pipeline\n",
        "\n",
        "## \ud83d\udccb Overview\n",
        "\n",
        "This **self-contained** notebook walks through the entire Geolife preprocessing pipeline from raw GPS data to final `.pk` training files.\n",
        "\n",
        "### Pipeline: 10 Steps\n",
        "\n",
        "**PART 1**: Raw Data Processing (Steps 1-8)\n",
        "1. Load GPS position fixes\n",
        "2. Generate staypoints (>30min, <200m)\n",
        "3. Detect activities (\u226525min)\n",
        "4. Filter users (>50 days)\n",
        "5. Cluster locations (DBSCAN, \u03b5=20m)\n",
        "6. Merge staypoints (gap <1min)\n",
        "7. Add time features\n",
        "8. Validate sequences\n",
        "\n",
        "**PART 2**: Transformer Data Generation (Steps 9-10)\n",
        "9. Split dataset (60/20/20)\n",
        "10. Generate .pk files\n",
        "\n",
        "### Expected Results\n",
        "- 45 users\n",
        "- 7,424 train sequences\n",
        "- 1,186 locations\n",
        "\n",
        "### Note\n",
        "Run cells sequentially. Adjust `raw_data_path` in config to your Geolife data location."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, sys, json, pickle, datetime\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import geopandas as gpd\n",
        "from shapely import wkt\n",
        "\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "from joblib import Parallel, delayed\n",
        "\n",
        "import trackintel as ti\n",
        "from trackintel.io.dataset_reader import read_geolife\n",
        "from trackintel.preprocessing.triplegs import generate_trips\n",
        "from trackintel.analysis.tracking_quality import temporal_tracking_quality, _split_overlaps\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "print('\u2713 Imports complete')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "CONFIG = {",
        "    'dataset': {",
        "        'name': 'geolife',",
        "        'raw_data_path': './data/geolife/Data',  # \u2190 ADJUST THIS PATH",
        "        'output_dir': './data/geolife'",
        "    },",
        "    'staypoints': {'dist_threshold': 200, 'time_threshold': 30, 'gap_threshold': 1440, 'include_last': True, 'print_progress': True, 'n_jobs': -1},",
        "    'activity_flag': {'method': 'time_threshold', 'time_threshold': 25},",
        "    'user_quality': {'day_filter': 50, 'window_size': 10, 'min_thres': None, 'mean_thres': None},",
        "    'locations': {'epsilon': 20, 'num_samples': 2, 'distance_metric': 'haversine', 'agg_level': 'dataset', 'n_jobs': -1},",
        "    'staypoint_merging': {'max_time_gap': '1min'},",
        "    'sequence_generation': {'previous_days': [7]},",
        "    'seed': 42",
        "}",
        "np.random.seed(CONFIG['seed'])",
        "os.makedirs(CONFIG['dataset']['output_dir'], exist_ok=True)",
        "os.makedirs(os.path.join(CONFIG['dataset']['output_dir'], 'quality'), exist_ok=True)",
        "print(f\"\u2713 Config: \u03b5={CONFIG['locations']['epsilon']}m, history={CONFIG['sequence_generation']['previous_days'][0]}days\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Utility Functions\n\nFunctions from `utils.py` - used throughout the pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import geopandas as gpd",
        "",
        "import pandas as pd",
        "",
        "from shapely import wkt",
        "from tqdm import tqdm",
        "",
        "",
        "import datetime",
        "",
        "from trackintel.analysis.tracking_quality import temporal_tracking_quality, _split_overlaps",
        "",
        "",
        "def preprocess_to_ti(df):",
        "    \"\"\"Change dataframe to trackintel compatible format\"\"\"",
        "    df.rename(",
        "        columns={\"userid\": \"user_id\", \"startt\": \"started_at\", \"endt\": \"finished_at\", \"dur_s\": \"duration\"},",
        "        inplace=True,",
        "    )",
        "",
        "    # read the time info",
        "    df[\"started_at\"] = pd.to_datetime(df[\"started_at\"])",
        "    df[\"finished_at\"] = pd.to_datetime(df[\"finished_at\"])",
        "    df[\"started_at\"] = df[\"started_at\"].dt.tz_localize(tz=\"utc\")",
        "    df[\"finished_at\"] = df[\"finished_at\"].dt.tz_localize(tz=\"utc\")",
        "",
        "    df[\"duration\"] = (df[\"finished_at\"] - df[\"started_at\"]).dt.total_seconds()",
        "    # drop invalid",
        "    df.drop(index=df[df[\"duration\"] < 0].index, inplace=True)",
        "",
        "    df.set_index(\"id\", inplace=True)",
        "    tqdm.pandas(desc=\"Load geometry\")",
        "    df[\"geom\"] = df[\"geom\"].progress_apply(wkt.loads)",
        "",
        "    return gpd.GeoDataFrame(df, crs=\"EPSG:4326\", geometry=\"geom\")",
        "",
        "",
        "def filter_duplicates(sp, tpls):",
        "",
        "    # merge trips and staypoints",
        "    sp[\"type\"] = \"sp\"",
        "    tpls[\"type\"] = \"tpl\"",
        "    df_all = pd.merge(sp, tpls, how=\"outer\")",
        "",
        "    df_all = df_all.groupby(\"user_id\", as_index=False).apply(_alter_diff)",
        "    sp = df_all.loc[df_all[\"type\"] == \"sp\"].drop(columns=[\"type\"])",
        "    tpls = df_all.loc[df_all[\"type\"] == \"tpl\"].drop(columns=[\"type\"])",
        "",
        "    sp = sp[[\"id\", \"user_id\", \"started_at\", \"finished_at\", \"geom\", \"duration\", \"is_activity\"]]",
        "    tpls = tpls[[\"id\", \"user_id\", \"started_at\", \"finished_at\", \"geom\", \"length_m\", \"duration\", \"mode\"]]",
        "",
        "    return sp.set_index(\"id\"), tpls.set_index(\"id\")",
        "",
        "",
        "def _alter_diff(df):",
        "    df.sort_values(by=\"started_at\", inplace=True)",
        "    df[\"diff\"] = pd.NA",
        "    df[\"st_next\"] = pd.NA",
        "",
        "    diff = df[\"started_at\"].iloc[1:].reset_index(drop=True) - df[\"finished_at\"].iloc[:-1].reset_index(drop=True)",
        "    df[\"diff\"].iloc[:-1] = diff.dt.total_seconds()",
        "    df[\"st_next\"].iloc[:-1] = df[\"started_at\"].iloc[1:].reset_index(drop=True)",
        "",
        "    df.loc[df[\"diff\"] < 0, \"finished_at\"] = df.loc[df[\"diff\"] < 0, \"st_next\"]",
        "",
        "    df[\"started_at\"], df[\"finished_at\"] = pd.to_datetime(df[\"started_at\"]), pd.to_datetime(df[\"finished_at\"])",
        "    df[\"duration\"] = (df[\"finished_at\"] - df[\"started_at\"]).dt.total_seconds()",
        "",
        "    # print(df.loc[df[\"diff\"] < 0])",
        "    df.drop(columns=[\"diff\", \"st_next\"], inplace=True)",
        "    df.drop(index=df[df[\"duration\"] <= 0].index, inplace=True)",
        "",
        "    return df",
        "",
        "",
        "def enrich_time_info(sp):",
        "    sp = sp.groupby(\"user_id\", group_keys=False).apply(_get_time)",
        "    sp.drop(columns={\"finished_at\", \"started_at\"}, inplace=True)",
        "    sp.sort_values(by=[\"user_id\", \"start_day\", \"start_min\"], inplace=True)",
        "    sp = sp.reset_index(drop=True)",
        "",
        "    #",
        "    sp[\"location_id\"] = sp[\"location_id\"].astype(int)",
        "    sp[\"user_id\"] = sp[\"user_id\"].astype(int)",
        "",
        "    # final cleaning, reassign ids",
        "    sp.index.name = \"id\"",
        "    sp.reset_index(inplace=True)",
        "    return sp",
        "",
        "",
        "def _get_time(df):",
        "    min_day = pd.to_datetime(df[\"started_at\"].min().date())",
        "    df[\"started_at\"] = df[\"started_at\"].dt.tz_localize(tz=None)",
        "    df[\"finished_at\"] = df[\"finished_at\"].dt.tz_localize(tz=None)",
        "",
        "    df[\"start_day\"] = (df[\"started_at\"] - min_day).dt.days",
        "    df[\"end_day\"] = (df[\"finished_at\"] - min_day).dt.days",
        "",
        "    df[\"start_min\"] = df[\"started_at\"].dt.hour * 60 + df[\"started_at\"].dt.minute",
        "    df[\"end_min\"] = df[\"finished_at\"].dt.hour * 60 + df[\"finished_at\"].dt.minute",
        "    df.loc[df[\"end_min\"] == 0, \"end_min\"] = 24 * 60",
        "",
        "    df[\"weekday\"] = df[\"started_at\"].dt.weekday",
        "    return df",
        "",
        "",
        "def calculate_user_quality(sp, trips, file_path, quality_filter):",
        "",
        "    trips[\"started_at\"] = pd.to_datetime(trips[\"started_at\"]).dt.tz_localize(None)",
        "    trips[\"finished_at\"] = pd.to_datetime(trips[\"finished_at\"]).dt.tz_localize(None)",
        "    sp[\"started_at\"] = pd.to_datetime(sp[\"started_at\"]).dt.tz_localize(None)",
        "    sp[\"finished_at\"] = pd.to_datetime(sp[\"finished_at\"]).dt.tz_localize(None)",
        "",
        "    # merge trips and staypoints",
        "    print(\"starting merge\", sp.shape, trips.shape)",
        "    sp[\"type\"] = \"sp\"",
        "    trips[\"type\"] = \"tpl\"",
        "    df_all = pd.concat([sp, trips])",
        "    df_all = _split_overlaps(df_all, granularity=\"day\")",
        "    df_all[\"duration\"] = (df_all[\"finished_at\"] - df_all[\"started_at\"]).dt.total_seconds()",
        "    print(\"finished merge\", df_all.shape)",
        "    print(\"*\" * 50)",
        "",
        "    if \"min_thres\" in quality_filter:",
        "        end_period = datetime.datetime(2017, 12, 26)",
        "        df_all = df_all.loc[df_all[\"finished_at\"] < end_period]",
        "",
        "    print(len(df_all[\"user_id\"].unique()))",
        "",
        "    # get quality",
        "    total_quality = temporal_tracking_quality(df_all, granularity=\"all\")",
        "    # get tracking days",
        "    total_quality[\"days\"] = (",
        "        df_all.groupby(\"user_id\").apply(lambda x: (x[\"finished_at\"].max() - x[\"started_at\"].min()).days).values",
        "    )",
        "    # filter based on days",
        "    user_filter_day = (",
        "        total_quality.loc[(total_quality[\"days\"] > quality_filter[\"day_filter\"])]",
        "        .reset_index(drop=True)[\"user_id\"]",
        "        .unique()",
        "    )",
        "",
        "    sliding_quality = (",
        "        df_all.groupby(\"user_id\")",
        "        .apply(_get_tracking_quality, window_size=quality_filter[\"window_size\"])",
        "        .reset_index(drop=True)",
        "    )",
        "",
        "    filter_after_day = sliding_quality.loc[sliding_quality[\"user_id\"].isin(user_filter_day)]",
        "",
        "    if \"min_thres\" in quality_filter:",
        "        # filter based on quanlity",
        "        filter_after_day = (",
        "            filter_after_day.groupby(\"user_id\")",
        "            .apply(_filter_user, min_thres=quality_filter[\"min_thres\"], mean_thres=quality_filter[\"mean_thres\"])",
        "            .reset_index(drop=True)",
        "            .dropna()",
        "        )",
        "",
        "    filter_after_user_quality = filter_after_day.groupby(\"user_id\", as_index=False)[\"quality\"].mean()",
        "",
        "    print(\"final selected user\", filter_after_user_quality.shape[0])",
        "    filter_after_user_quality.to_csv(file_path, index=False)",
        "    return filter_after_user_quality[\"user_id\"].values",
        "",
        "",
        "def _filter_user(df, min_thres, mean_thres):",
        "    consider = df.loc[df[\"quality\"] != 0]",
        "    if (consider[\"quality\"].min() > min_thres) and (consider[\"quality\"].mean() > mean_thres):",
        "        return df",
        "",
        "",
        "def _get_tracking_quality(df, window_size):",
        "",
        "    weeks = (df[\"finished_at\"].max() - df[\"started_at\"].min()).days // 7",
        "    start_date = df[\"started_at\"].min().date()",
        "",
        "    quality_list = []",
        "    # construct the sliding week gdf",
        "    for i in range(0, weeks - window_size + 1):",
        "        curr_start = datetime.datetime.combine(start_date + datetime.timedelta(weeks=i), datetime.time())",
        "        curr_end = datetime.datetime.combine(curr_start + datetime.timedelta(weeks=window_size), datetime.time())",
        "",
        "        # the total df for this time window",
        "        cAll_gdf = df.loc[(df[\"started_at\"] >= curr_start) & (df[\"finished_at\"] < curr_end)]",
        "        if cAll_gdf.shape[0] == 0:",
        "            continue",
        "        total_sec = (curr_end - curr_start).total_seconds()",
        "",
        "        quality_list.append([i, cAll_gdf[\"duration\"].sum() / total_sec])",
        "    ret = pd.DataFrame(quality_list, columns=[\"timestep\", \"quality\"])",
        "    ret[\"user_id\"] = df[\"user_id\"].unique()[0]",
        "    return ret",
        "",
        "",
        "def split_dataset(totalData):",
        "    \"\"\"Split dataset into train, vali and test.\"\"\"",
        "    totalData = totalData.groupby(\"user_id\",group_keys=False).apply(_get_split_days_user)",
        "",
        "    train_data = totalData.loc[totalData[\"Dataset\"] == \"train\"].copy()",
        "    vali_data = totalData.loc[totalData[\"Dataset\"] == \"vali\"].copy()",
        "    test_data = totalData.loc[totalData[\"Dataset\"] == \"test\"].copy()",
        "",
        "    # final cleaning",
        "    train_data.drop(columns={\"Dataset\"}, inplace=True)",
        "    vali_data.drop(columns={\"Dataset\"}, inplace=True)",
        "    test_data.drop(columns={\"Dataset\"}, inplace=True)",
        "",
        "    return train_data, vali_data, test_data",
        "",
        "",
        "def _get_split_days_user(df):",
        "    \"\"\"Split the dataset according to the tracked day of each user.\"\"\"",
        "    maxDay = df[\"start_day\"].max()",
        "    train_split = maxDay * 0.6",
        "    validation_split = maxDay * 0.8",
        "",
        "    df[\"Dataset\"] = \"test\"",
        "    df.loc[df[\"start_day\"] < train_split, \"Dataset\"] = \"train\"",
        "    df.loc[(df[\"start_day\"] >= train_split) & (df[\"start_day\"] < validation_split), \"Dataset\"] = \"vali\"",
        "",
        "    return df",
        "",
        "def get_valid_sequence(input_df, previous_day=14):",
        "",
        "    valid_id = []",
        "    for user in input_df[\"user_id\"].unique():",
        "        df = input_df.loc[input_df[\"user_id\"] == user].copy().reset_index(drop=True)",
        "",
        "        min_days = df[\"start_day\"].min()",
        "        df[\"diff_day\"] = df[\"start_day\"] - min_days",
        "",
        "        for index, row in df.iterrows():",
        "            # exclude the first records",
        "            if row[\"diff_day\"] < previous_day:",
        "                continue",
        "",
        "            hist = df.iloc[:index]",
        "            hist = hist.loc[(hist[\"start_day\"] >= (row[\"start_day\"] - previous_day))]",
        "            if len(hist) < 3:",
        "                continue",
        "",
        "            valid_id.append(row[\"id\"])",
        "",
        "    return valid_id"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PART 1: Raw Data Preprocessing\n\n---\n\n## Step 1: Load Geolife Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('='*80)\n",
        "print('LOADING GEOLIFE DATA')\n",
        "print('='*80)\n",
        "\n",
        "pfs, _ = read_geolife(CONFIG['dataset']['raw_data_path'], print_progress=True)\n",
        "print(f'\\n\u2713 Loaded {len(pfs)} position fixes from {pfs[\"user_id\"].nunique()} users')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Generate Staypoints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('='*80)\n",
        "print('GENERATING STAYPOINTS')\n",
        "print('='*80)\n",
        "\n",
        "pfs, sp = pfs.as_positionfixes.generate_staypoints(\n",
        "    gap_threshold=CONFIG['staypoints']['gap_threshold'],\n",
        "    include_last=CONFIG['staypoints']['include_last'],\n",
        "    print_progress=CONFIG['staypoints']['print_progress'],\n",
        "    dist_threshold=CONFIG['staypoints']['dist_threshold'],\n",
        "    time_threshold=CONFIG['staypoints']['time_threshold'],\n",
        "    n_jobs=CONFIG['staypoints']['n_jobs']\n",
        ")\n",
        "print(f'\\n\u2713 Generated {len(sp)} staypoints')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Create Activity Flags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sp = sp.as_staypoints.create_activity_flag(\n",
        "    method=CONFIG['activity_flag']['method'],\n",
        "    time_threshold=CONFIG['activity_flag']['time_threshold']\n",
        ")\n",
        "print(f'\u2713 {sp[\"is_activity\"].sum()} / {len(sp)} are activities')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Filter Users by Quality"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('='*80)\n",
        "print('USER QUALITY FILTERING')\n",
        "print('='*80)\n",
        "\n",
        "quality_file = os.path.join(CONFIG['dataset']['output_dir'], 'quality', 'geolife_slide_filtered.csv')\n",
        "\n",
        "if Path(quality_file).is_file():\n",
        "    valid_user = pd.read_csv(quality_file)['user_id'].values\n",
        "    print(f'\u2713 Loaded {len(valid_user)} users from existing quality file')\n",
        "else:\n",
        "    print('Generating quality metrics...')\n",
        "    pfs, tpls = pfs.as_positionfixes.generate_triplegs(sp)\n",
        "    sp, tpls, trips = generate_trips(sp, tpls, add_geometry=False)\n",
        "    \n",
        "    quality_filter = {\n",
        "        'day_filter': CONFIG['user_quality']['day_filter'],\n",
        "        'window_size': CONFIG['user_quality']['window_size']\n",
        "    }\n",
        "    if CONFIG['user_quality'].get('min_thres') is not None:\n",
        "        quality_filter['min_thres'] = CONFIG['user_quality']['min_thres']\n",
        "        quality_filter['mean_thres'] = CONFIG['user_quality']['mean_thres']\n",
        "    \n",
        "    valid_user = calculate_user_quality(sp.copy(), trips.copy(), quality_filter)\n",
        "    pd.DataFrame({'user_id': valid_user}).to_csv(quality_file, index=False)\n",
        "    print(f'\u2713 Saved quality file')\n",
        "\n",
        "sp = sp.loc[sp['user_id'].isin(valid_user)]\n",
        "print(f'\\n\u2713 {len(valid_user)} valid users, {len(sp)} staypoints')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Keep Only Activities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sp = sp.loc[sp['is_activity'] == True]\n",
        "print(f'\u2713 Filtered to {len(sp)} activity staypoints')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Generate Locations (DBSCAN Clustering)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('='*80)\n",
        "print('GENERATING LOCATIONS')\n",
        "print('='*80)\n",
        "\n",
        "sp, locs = sp.as_staypoints.generate_locations(\n",
        "    epsilon=CONFIG['locations']['epsilon'],\n",
        "    num_samples=CONFIG['locations']['num_samples'],\n",
        "    distance_metric=CONFIG['locations']['distance_metric'],\n",
        "    agg_level=CONFIG['locations']['agg_level'],\n",
        "    n_jobs=CONFIG['locations']['n_jobs']\n",
        ")\n",
        "\n",
        "sp = sp.loc[~sp['location_id'].isna()].copy()\n",
        "print(f'\u2713 {sp[\"location_id\"].nunique()} unique locations in {len(sp)} staypoints')\n",
        "\n",
        "# Save locations\n",
        "locs = locs[~locs.index.duplicated(keep='first')]\n",
        "filtered_locs = locs.loc[locs.index.isin(sp['location_id'].unique())]\n",
        "filtered_locs.as_locations.to_csv(os.path.join(CONFIG['dataset']['output_dir'], 'locations_geolife.csv'))\n",
        "print(f'\u2713 Saved locations')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Merge Staypoints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sp = sp[['user_id', 'started_at', 'finished_at', 'geom', 'location_id']]\n",
        "sp_merged = sp.as_staypoints.merge_staypoints(\n",
        "    triplegs=pd.DataFrame([]),\n",
        "    max_time_gap=CONFIG['staypoint_merging']['max_time_gap'],\n",
        "    agg={'location_id': 'first'}\n",
        ")\n",
        "sp_merged['duration'] = (sp_merged['finished_at'] - sp_merged['started_at']).dt.total_seconds() // 60\n",
        "print(f'\u2713 Merged: {len(sp)} \u2192 {len(sp_merged)} staypoints')\n",
        "sp = sp_merged"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: Add Temporal Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sp_time = enrich_time_info(sp)\n",
        "print(f'\u2713 Added temporal features: {len(sp_time)} staypoints, {sp_time[\"user_id\"].nunique()} users')\n",
        "sp_time.to_csv(os.path.join(CONFIG['dataset']['output_dir'], 'sp_time_temp_geolife.csv'), index=False)\n",
        "print(f'\u2713 Saved intermediate result')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 9: Sequence Validation & Final Filtering\n",
        "\n",
        "Find staypoints with sufficient historical context and filter users to ensure all splits have data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('='*80)\n",
        "print('SEQUENCE VALIDATION & FINAL FILTERING')\n",
        "print('='*80)\n",
        "\n",
        "# Split dataset\n",
        "train_data, vali_data, test_data = split_dataset(sp_time)\n",
        "\n",
        "# Encode locations\n",
        "enc = OrdinalEncoder(dtype=np.int64, handle_unknown='use_encoded_value', unknown_value=-1).fit(\n",
        "    train_data['location_id'].values.reshape(-1, 1)\n",
        ")\n",
        "train_data['location_id'] = enc.transform(train_data['location_id'].values.reshape(-1, 1)) + 2\n",
        "vali_data['location_id'] = enc.transform(vali_data['location_id'].values.reshape(-1, 1)) + 2\n",
        "test_data['location_id'] = enc.transform(test_data['location_id'].values.reshape(-1, 1)) + 2\n",
        "\n",
        "# Find valid sequences for each previous_day\n",
        "previous_day_ls = CONFIG['sequence_generation']['previous_days']\n",
        "all_ids = sp_time[['id']].copy()\n",
        "\n",
        "for previous_day in tqdm(previous_day_ls, desc='Finding valid sequences'):\n",
        "    valid_ids = get_valid_sequence(train_data, previous_day=previous_day)\n",
        "    valid_ids.extend(get_valid_sequence(vali_data, previous_day=previous_day))\n",
        "    valid_ids.extend(get_valid_sequence(test_data, previous_day=previous_day))\n",
        "    \n",
        "    all_ids[f'{previous_day}'] = 0\n",
        "    all_ids.loc[all_ids['id'].isin(valid_ids), f'{previous_day}'] = 1\n",
        "\n",
        "# Get final valid IDs (valid for all previous_day values)\n",
        "all_ids.set_index('id', inplace=True)\n",
        "final_valid_id = all_ids.loc[all_ids.sum(axis=1) == all_ids.shape[1]].reset_index()['id'].values\n",
        "\n",
        "# Filter users: must have records in all splits\n",
        "valid_users_train = train_data.loc[train_data['id'].isin(final_valid_id), 'user_id'].unique()\n",
        "valid_users_vali = vali_data.loc[vali_data['id'].isin(final_valid_id), 'user_id'].unique()\n",
        "valid_users_test = test_data.loc[test_data['id'].isin(final_valid_id), 'user_id'].unique()\n",
        "valid_users = set.intersection(set(valid_users_train), set(valid_users_vali), set(valid_users_test))\n",
        "\n",
        "filtered_sp = sp_time.loc[sp_time['user_id'].isin(valid_users)].copy()\n",
        "\n",
        "# Re-split filtered data\n",
        "train_data, vali_data, test_data = split_dataset(filtered_sp)\n",
        "\n",
        "# Re-encode locations\n",
        "enc = OrdinalEncoder(dtype=np.int64, handle_unknown='use_encoded_value', unknown_value=-1).fit(\n",
        "    train_data['location_id'].values.reshape(-1, 1)\n",
        ")\n",
        "train_data['location_id'] = enc.transform(train_data['location_id'].values.reshape(-1, 1)) + 2\n",
        "\n",
        "print(f'\u2713 Max location ID: {train_data[\"location_id\"].max()}')\n",
        "print(f'\u2713 Unique locations: {train_data[\"location_id\"].nunique()}')\n",
        "\n",
        "# Re-encode user IDs (continuous)\n",
        "enc_user = OrdinalEncoder(dtype=np.int64)\n",
        "filtered_sp['user_id'] = enc_user.fit_transform(filtered_sp['user_id'].values.reshape(-1, 1)) + 1\n",
        "\n",
        "# Save\n",
        "with open(os.path.join(CONFIG['dataset']['output_dir'], 'valid_ids_geolife.pk'), 'wb') as f:\n",
        "    pickle.dump(final_valid_id, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "filtered_sp.to_csv(os.path.join(CONFIG['dataset']['output_dir'], 'dataSet_geolife.csv'), index=False)\n",
        "\n",
        "print(f'\\n\u2713 Final: {filtered_sp[\"user_id\"].nunique()} users, {len(filtered_sp)} staypoints')\n",
        "print(f'\u2713 Saved: dataSet_geolife.csv and valid_ids_geolife.pk')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# PART 2: Transformer Data Generation\n",
        "\n",
        "Generate final `.pk` files for model training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Helper Functions for Sequence Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_valid_sequence_user(df, previous_day, valid_ids):\n",
        "    \"\"\"Get valid sequences for a single user.\"\"\"\n",
        "    df.reset_index(drop=True, inplace=True)\n",
        "    data_single_user = []\n",
        "    \n",
        "    min_days = df['start_day'].min()\n",
        "    df['diff_day'] = df['start_day'] - min_days\n",
        "    \n",
        "    for index, row in df.iterrows():\n",
        "        if row['diff_day'] < previous_day:\n",
        "            continue\n",
        "        \n",
        "        hist = df.iloc[:index]\n",
        "        hist = hist.loc[(hist['start_day'] >= (row['start_day'] - previous_day))]\n",
        "        \n",
        "        if not (row['id'] in valid_ids):\n",
        "            continue\n",
        "        \n",
        "        if len(hist) < 2:\n",
        "            continue\n",
        "        \n",
        "        data_dict = {}\n",
        "        data_dict['X'] = hist['location_id'].values\n",
        "        data_dict['user_X'] = hist['user_id'].values\n",
        "        data_dict['weekday_X'] = hist['weekday'].values\n",
        "        data_dict['start_min_X'] = hist['start_min'].values\n",
        "        data_dict['dur_X'] = hist['duration'].values\n",
        "        data_dict['diff'] = (row['diff_day'] - hist['diff_day']).astype(int).values\n",
        "        data_dict['Y'] = int(row['location_id'])\n",
        "        \n",
        "        data_single_user.append(data_dict)\n",
        "    \n",
        "    return data_single_user\n",
        "\n",
        "\n",
        "def apply_parallel(dfGrouped, func, n_jobs, **kwargs):\n",
        "    \"\"\"Parallelize functions after groupby.\"\"\"\n",
        "    if n_jobs == 1:\n",
        "        return dfGrouped.apply(func, **kwargs)\n",
        "    \n",
        "    results = Parallel(n_jobs=n_jobs)(\n",
        "        delayed(func)(group, **kwargs) for name, group in dfGrouped\n",
        "    )\n",
        "    return results\n",
        "\n",
        "\n",
        "print('\u2713 Sequence generation functions defined')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 10: Generate Final .pk Files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('='*80)\n",
        "print('GENERATING TRANSFORMER DATA FILES')\n",
        "print('='*80)\n",
        "\n",
        "# Load preprocessed data\n",
        "dataset_path = os.path.join(CONFIG['dataset']['output_dir'], 'dataSet_geolife.csv')\n",
        "valid_ids_path = os.path.join(CONFIG['dataset']['output_dir'], 'valid_ids_geolife.pk')\n",
        "\n",
        "ori_data = pd.read_csv(dataset_path)\n",
        "with open(valid_ids_path, 'rb') as f:\n",
        "    valid_ids = pickle.load(f)\n",
        "\n",
        "# Sort\n",
        "ori_data.sort_values(by=['user_id', 'start_day', 'start_min'], inplace=True)\n",
        "\n",
        "# Truncate long durations (>2 days \u2192 2 days)\n",
        "ori_data.loc[ori_data['duration'] > 60 * 24 * 2 - 1, 'duration'] = 60 * 24 * 2 - 1\n",
        "\n",
        "# Split\n",
        "train_data, vali_data, test_data = split_dataset(ori_data)\n",
        "print(f'Split: train={len(train_data)}, val={len(vali_data)}, test={len(test_data)}')\n",
        "\n",
        "# Encode locations\n",
        "enc = OrdinalEncoder(dtype=np.int64, handle_unknown='use_encoded_value', unknown_value=-1).fit(\n",
        "    train_data['location_id'].values.reshape(-1, 1)\n",
        ")\n",
        "train_data['location_id'] = enc.transform(train_data['location_id'].values.reshape(-1, 1)) + 2\n",
        "vali_data['location_id'] = enc.transform(vali_data['location_id'].values.reshape(-1, 1)) + 2\n",
        "test_data['location_id'] = enc.transform(test_data['location_id'].values.reshape(-1, 1)) + 2\n",
        "\n",
        "print(f'Max location ID: {train_data[\"location_id\"].max()}')\n",
        "print(f'Unique locations: {train_data[\"location_id\"].nunique()}')\n",
        "\n",
        "# Generate sequences\n",
        "previous_day = CONFIG['sequence_generation']['previous_days'][0]\n",
        "print(f'\\nGenerating sequences (history={previous_day} days)...')\n",
        "\n",
        "train_records = apply_parallel(\n",
        "    train_data.groupby('user_id'),\n",
        "    get_valid_sequence_user,\n",
        "    n_jobs=-1,\n",
        "    previous_day=previous_day,\n",
        "    valid_ids=valid_ids\n",
        ")\n",
        "train_records = [item for sublist in train_records for item in sublist]\n",
        "print(f'Train: {len(train_records)} sequences')\n",
        "\n",
        "vali_records = apply_parallel(\n",
        "    vali_data.groupby('user_id'),\n",
        "    get_valid_sequence_user,\n",
        "    n_jobs=-1,\n",
        "    previous_day=previous_day,\n",
        "    valid_ids=valid_ids\n",
        ")\n",
        "vali_records = [item for sublist in vali_records for item in sublist]\n",
        "print(f'Validation: {len(vali_records)} sequences')\n",
        "\n",
        "test_records = apply_parallel(\n",
        "    test_data.groupby('user_id'),\n",
        "    get_valid_sequence_user,\n",
        "    n_jobs=-1,\n",
        "    previous_day=previous_day,\n",
        "    valid_ids=valid_ids\n",
        ")\n",
        "test_records = [item for sublist in test_records for item in sublist]\n",
        "print(f'Test: {len(test_records)} sequences')\n",
        "\n",
        "# Save .pk files\n",
        "output_dir = CONFIG['dataset']['output_dir']\n",
        "train_file = os.path.join(output_dir, f'geolife_transformer_{previous_day}_train.pk')\n",
        "vali_file = os.path.join(output_dir, f'geolife_transformer_{previous_day}_validation.pk')\n",
        "test_file = os.path.join(output_dir, f'geolife_transformer_{previous_day}_test.pk')\n",
        "\n",
        "with open(train_file, 'wb') as f:\n",
        "    pickle.dump(train_records, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "print(f'\\n\u2713 Saved: {train_file}')\n",
        "\n",
        "with open(vali_file, 'wb') as f:\n",
        "    pickle.dump(vali_records, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "print(f'\u2713 Saved: {vali_file}')\n",
        "\n",
        "with open(test_file, 'wb') as f:\n",
        "    pickle.dump(test_records, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "print(f'\u2713 Saved: {test_file}')\n",
        "\n",
        "print('\\n' + '='*80)\n",
        "print('PREPROCESSING COMPLETE!')\n",
        "print('='*80)\n",
        "print(f'\\nFinal output files in {output_dir}:')\n",
        "print(f'  - geolife_transformer_{previous_day}_train.pk ({len(train_records)} sequences)')\n",
        "print(f'  - geolife_transformer_{previous_day}_validation.pk ({len(vali_records)} sequences)')\n",
        "print(f'  - geolife_transformer_{previous_day}_test.pk ({len(test_records)} sequences)')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}