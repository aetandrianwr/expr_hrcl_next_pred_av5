{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Complete HistoryCentricModel Training Walkthrough\n",
        "\n",
        "This notebook provides a **complete, self-contained walkthrough** of training the HistoryCentricModel for next-location prediction. The notebook is designed to be fully executable without any dependencies on external project scripts.\n",
        "\n",
        "## Overview\n",
        "\n",
        "This notebook walks through the entire training pipeline from start to finish:\n",
        "\n",
        "1. **Data Loading**: Reading preprocessed pickle files containing trajectory sequences\n",
        "2. **Model Architecture**: Implementing the HistoryCentricModel from scratch\n",
        "3. **Training Loop**: Setting up optimizer, loss functions, and training procedures\n",
        "4. **Evaluation**: Computing comprehensive metrics on test data\n",
        "5. **Results**: Displaying final performance metrics\n",
        "\n",
        "## Model Architecture: HistoryCentricModel\n",
        "\n",
        "The **HistoryCentricModel** is designed based on a key insight: **83.81% of next locations are already in the visit history**. The model combines:\n",
        "\n",
        "- **History-based scoring**: Uses recency (exponential decay) and frequency patterns from visit history\n",
        "- **Learned patterns**: A compact transformer to learn temporal context and transition patterns\n",
        "- **Ensemble approach**: Combines history scores with learned model outputs\n",
        "\n",
        "The model is highly parameter-efficient (<500K parameters) while achieving strong performance.\n",
        "\n",
        "## Dataset\n",
        "\n",
        "We use the **GeoLife** trajectory dataset, which contains:\n",
        "- Location sequences from multiple users\n",
        "- Temporal features (time of day, day of week, duration, time gaps)\n",
        "- User identifiers\n",
        "- Target: Next location to visit\n",
        "\n",
        "Each sample includes:\n",
        "- `X`: Location ID sequence\n",
        "- `user_X`: User ID for each location  \n",
        "- `weekday_X`: Day of week (0-6)\n",
        "- `start_min_X`: Start time in minutes from midnight\n",
        "- `dur_X`: Duration at each location (minutes)\n",
        "- `diff`: Time gap indicator\n",
        "- `Y`: Target next location\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Imports\n",
        "\n",
        "First, we import all necessary libraries. This notebook is self-contained and only uses standard deep learning libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "import numpy as np\n",
        "import math\n",
        "import time\n",
        "from tqdm.notebook import tqdm\n",
        "from sklearn.metrics import f1_score\n",
        "import random\n",
        "\n",
        "print(\"PyTorch version:\", torch.__version__)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"CUDA device:\", torch.cuda.get_device_name(0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Configuration Parameters\n",
        "\n",
        "We define all hyperparameters and paths in one place for easy modification. These values match the production configuration from `configs/geolife_default.yaml`.\n",
        "\n",
        "### Key Parameters:\n",
        "- **Data**: Paths to train/val/test pickle files, batch size, sequence length\n",
        "- **Model**: Embedding dimensions, transformer architecture (d_model=80, 1 layer, 4 heads)\n",
        "- **Training**: Learning rate, weight decay, epochs, early stopping\n",
        "- **System**: Device (GPU/CPU), random seed for reproducibility"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration dictionary\n",
        "config = {\n",
        "    # Data paths and parameters\n",
        "    'data': {\n",
        "        'data_dir': '/content/expr_hrcl_next_pred_av5/data/geolife',\n",
        "        'train_file': 'geolife_transformer_7_train.pk',\n",
        "        'val_file': 'geolife_transformer_7_validation.pk',\n",
        "        'test_file': 'geolife_transformer_7_test.pk',\n",
        "        'num_locations': 1187,  # 1186 + 1 for padding\n",
        "        'num_users': 46,  # 45 + 1 for padding\n",
        "        'num_weekdays': 7,\n",
        "        'max_seq_len': 60,\n",
        "    },\n",
        "    \n",
        "    # Model architecture\n",
        "    'model': {\n",
        "        'name': 'HistoryCentricModel',\n",
        "        'loc_emb_dim': 56,\n",
        "        'user_emb_dim': 12,\n",
        "        'weekday_emb_dim': 4,\n",
        "        'time_emb_dim': 12,\n",
        "        'd_model': 80,\n",
        "        'nhead': 4,\n",
        "        'num_layers': 1,\n",
        "        'dim_feedforward': 160,\n",
        "        'dropout': 0.35,\n",
        "    },\n",
        "    \n",
        "    # Training parameters\n",
        "    'training': {\n",
        "        'batch_size': 96,\n",
        "        'num_epochs': 120,\n",
        "        'learning_rate': 0.0025,\n",
        "        'weight_decay': 0.00008,\n",
        "        'grad_clip': 1.0,\n",
        "        'label_smoothing': 0.02,\n",
        "        'early_stopping_patience': 20,\n",
        "        'lr_scheduler_patience': 10,\n",
        "        'lr_scheduler_factor': 0.6,\n",
        "        'min_lr': 5e-7,\n",
        "    },\n",
        "    \n",
        "    # System\n",
        "    'system': {\n",
        "        'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
        "        'num_workers': 2,\n",
        "        'seed': 42,\n",
        "    }\n",
        "}\n",
        "\n",
        "print(\"Configuration loaded successfully!\")\n",
        "print(f\"Device: {config['system']['device']}\")\n",
        "print(f\"Batch size: {config['training']['batch_size']}\")\n",
        "print(f\"Max epochs: {config['training']['num_epochs']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Set Random Seed for Reproducibility\n",
        "\n",
        "Setting random seeds ensures that results are reproducible across runs. We set seeds for:\n",
        "- Python's random module\n",
        "- NumPy\n",
        "- PyTorch (CPU and CUDA)\n",
        "- CuDNN backend for deterministic behavior"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def set_seed(seed):\n",
        "    \"\"\"Set random seeds for reproducibility.\"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed(config['system']['seed'])\n",
        "print(f\"Random seed set to {config['system']['seed']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Dataset Implementation\n",
        "\n",
        "The `GeoLifeDataset` class handles loading and preparing trajectory data:\n",
        "\n",
        "### Data Structure:\n",
        "Each sample in the pickle file contains:\n",
        "- **X**: Array of location IDs (sequence)\n",
        "- **user_X**: User ID for each visit\n",
        "- **weekday_X**: Day of week (0=Monday, 6=Sunday)\n",
        "- **start_min_X**: Start time in minutes from midnight (0-1439)\n",
        "- **dur_X**: Duration at each location in minutes\n",
        "- **diff**: Time gap indicator between visits\n",
        "- **Y**: Target next location (ground truth)\n",
        "\n",
        "### Processing:\n",
        "- Sequences longer than `max_seq_len` are truncated (keeping most recent)\n",
        "- Each feature is converted to appropriate PyTorch tensor type\n",
        "- Variable-length sequences are handled in the collate function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class GeoLifeDataset(Dataset):\n",
        "    \"\"\"Dataset for GeoLife trajectory sequences.\"\"\"\n",
        "    \n",
        "    def __init__(self, data_path, max_seq_len=60):\n",
        "        with open(data_path, 'rb') as f:\n",
        "            self.data = pickle.load(f)\n",
        "        self.max_seq_len = max_seq_len\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.data[idx]\n",
        "        \n",
        "        # Extract features\n",
        "        loc_seq = sample['X']\n",
        "        user_seq = sample['user_X']\n",
        "        weekday_seq = sample['weekday_X']\n",
        "        start_min_seq = sample['start_min_X']\n",
        "        dur_seq = sample['dur_X']\n",
        "        diff_seq = sample['diff']\n",
        "        target = sample['Y']\n",
        "        \n",
        "        # Truncate if too long (keep most recent)\n",
        "        seq_len = len(loc_seq)\n",
        "        if seq_len > self.max_seq_len:\n",
        "            loc_seq = loc_seq[-self.max_seq_len:]\n",
        "            user_seq = user_seq[-self.max_seq_len:]\n",
        "            weekday_seq = weekday_seq[-self.max_seq_len:]\n",
        "            start_min_seq = start_min_seq[-self.max_seq_len:]\n",
        "            dur_seq = dur_seq[-self.max_seq_len:]\n",
        "            diff_seq = diff_seq[-self.max_seq_len:]\n",
        "            seq_len = self.max_seq_len\n",
        "        \n",
        "        return {\n",
        "            'loc_seq': torch.LongTensor(loc_seq),\n",
        "            'user_seq': torch.LongTensor(user_seq),\n",
        "            'weekday_seq': torch.LongTensor(weekday_seq),\n",
        "            'start_min_seq': torch.FloatTensor(start_min_seq),\n",
        "            'dur_seq': torch.FloatTensor(dur_seq),\n",
        "            'diff_seq': torch.LongTensor(diff_seq),\n",
        "            'target': torch.LongTensor([target]),\n",
        "            'seq_len': seq_len\n",
        "        }\n",
        "\n",
        "print(\"Dataset class defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Collate Function for Batching\n",
        "\n",
        "Since sequences have variable lengths, we need a custom collate function:\n",
        "- Finds the maximum sequence length in the batch\n",
        "- Pads all sequences to this length with zeros\n",
        "- Creates an attention mask (1 for real tokens, 0 for padding)\n",
        "- Stacks all samples into batch tensors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def collate_fn(batch):\n",
        "    \"\"\"Custom collate function to handle variable-length sequences.\"\"\"\n",
        "    max_len = max(item['seq_len'] for item in batch)\n",
        "    batch_size = len(batch)\n",
        "    \n",
        "    # Initialize padded tensors\n",
        "    loc_seqs = torch.zeros(batch_size, max_len, dtype=torch.long)\n",
        "    user_seqs = torch.zeros(batch_size, max_len, dtype=torch.long)\n",
        "    weekday_seqs = torch.zeros(batch_size, max_len, dtype=torch.long)\n",
        "    start_min_seqs = torch.zeros(batch_size, max_len, dtype=torch.float)\n",
        "    dur_seqs = torch.zeros(batch_size, max_len, dtype=torch.float)\n",
        "    diff_seqs = torch.zeros(batch_size, max_len, dtype=torch.long)\n",
        "    targets = torch.zeros(batch_size, dtype=torch.long)\n",
        "    seq_lens = torch.zeros(batch_size, dtype=torch.long)\n",
        "    \n",
        "    # Fill in the data\n",
        "    for i, item in enumerate(batch):\n",
        "        length = item['seq_len']\n",
        "        loc_seqs[i, :length] = item['loc_seq']\n",
        "        user_seqs[i, :length] = item['user_seq']\n",
        "        weekday_seqs[i, :length] = item['weekday_seq']\n",
        "        start_min_seqs[i, :length] = item['start_min_seq']\n",
        "        dur_seqs[i, :length] = item['dur_seq']\n",
        "        diff_seqs[i, :length] = item['diff_seq']\n",
        "        targets[i] = item['target']\n",
        "        seq_lens[i] = length\n",
        "    \n",
        "    # Create attention mask (1 for real tokens, 0 for padding)\n",
        "    mask = torch.arange(max_len).unsqueeze(0) < seq_lens.unsqueeze(1)\n",
        "    \n",
        "    return {\n",
        "        'loc_seq': loc_seqs,\n",
        "        'user_seq': user_seqs,\n",
        "        'weekday_seq': weekday_seqs,\n",
        "        'start_min_seq': start_min_seqs,\n",
        "        'dur_seq': dur_seqs,\n",
        "        'diff_seq': diff_seqs,\n",
        "        'target': targets,\n",
        "        'mask': mask,\n",
        "        'seq_len': seq_lens\n",
        "    }\n",
        "\n",
        "print(\"Collate function defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Load Datasets\n",
        "\n",
        "We create DataLoaders for train, validation, and test sets:\n",
        "- **Training**: Shuffled for better learning\n",
        "- **Validation/Test**: Not shuffled for consistent evaluation\n",
        "- **pin_memory**: Faster GPU transfer if using CUDA\n",
        "\n",
        "The datasets are loaded from preprocessed pickle files containing trajectory sequences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Paths to data files\n",
        "data_dir = config['data']['data_dir']\n",
        "train_path = os.path.join(data_dir, config['data']['train_file'])\n",
        "val_path = os.path.join(data_dir, config['data']['val_file'])\n",
        "test_path = os.path.join(data_dir, config['data']['test_file'])\n",
        "\n",
        "# Create datasets\n",
        "print(\"Loading datasets...\")\n",
        "train_dataset = GeoLifeDataset(train_path, max_seq_len=config['data']['max_seq_len'])\n",
        "val_dataset = GeoLifeDataset(val_path, max_seq_len=config['data']['max_seq_len'])\n",
        "test_dataset = GeoLifeDataset(test_path, max_seq_len=config['data']['max_seq_len'])\n",
        "\n",
        "print(f\"Train samples: {len(train_dataset)}\")\n",
        "print(f\"Validation samples: {len(val_dataset)}\")\n",
        "print(f\"Test samples: {len(test_dataset)}\")\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=config['training']['batch_size'],\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_fn,\n",
        "    num_workers=config['system']['num_workers'],\n",
        "    pin_memory=True if config['system']['device'] == 'cuda' else False\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=config['training']['batch_size'],\n",
        "    shuffle=False,\n",
        "    collate_fn=collate_fn,\n",
        "    num_workers=config['system']['num_workers'],\n",
        "    pin_memory=True if config['system']['device'] == 'cuda' else False\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=config['training']['batch_size'],\n",
        "    shuffle=False,\n",
        "    collate_fn=collate_fn,\n",
        "    num_workers=config['system']['num_workers'],\n",
        "    pin_memory=True if config['system']['device'] == 'cuda' else False\n",
        ")\n",
        "\n",
        "print(f\"\\nTrain batches: {len(train_loader)}\")\n",
        "print(f\"Val batches: {len(val_loader)}\")\n",
        "print(f\"Test batches: {len(test_loader)}\")\n",
        "\n",
        "# Inspect a sample batch\n",
        "sample_batch = next(iter(train_loader))\n",
        "print(f\"\\nSample batch shape:\")\n",
        "print(f\"  loc_seq: {sample_batch['loc_seq'].shape}\")\n",
        "print(f\"  user_seq: {sample_batch['user_seq'].shape}\")\n",
        "print(f\"  target: {sample_batch['target'].shape}\")\n",
        "print(f\"  mask: {sample_batch['mask'].shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Model Implementation: HistoryCentricModel\n",
        "\n",
        "The **HistoryCentricModel** is the core of our next-location prediction system. It combines:\n",
        "\n",
        "### Architecture Components:\n",
        "\n",
        "1. **Embeddings**:\n",
        "   - Location embeddings (56-dim)\n",
        "   - User embeddings (12-dim)\n",
        "   - Temporal features encoded via sinusoidal functions\n",
        "\n",
        "2. **Compact Transformer**:\n",
        "   - Single layer with 4 attention heads\n",
        "   - d_model = 80 (very compact)\n",
        "   - Feedforward dimension = 160\n",
        "   - Dropout = 0.35 for regularization\n",
        "\n",
        "3. **History Scoring Mechanism**:\n",
        "   - **Recency**: Exponential decay based on how recently a location was visited\n",
        "   - **Frequency**: How often a location appears in the sequence\n",
        "   - Learnable weights to balance these factors\n",
        "\n",
        "4. **Ensemble Strategy**:\n",
        "   - Combines history scores with learned transformer outputs\n",
        "   - Learnable weight balances history vs learned patterns\n",
        "\n",
        "### Key Insight:\n",
        "Since 83.81% of next locations are already in visit history, the model heavily weights historical patterns while still learning complex temporal dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class HistoryCentricModel(nn.Module):\n",
        "    \"\"\"Model that heavily prioritizes locations from visit history.\"\"\"\n",
        "    \n",
        "    def __init__(self, num_locations, num_users, max_seq_len=60):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.num_locations = num_locations\n",
        "        self.d_model = 80  # Compact\n",
        "        \n",
        "        # Core embeddings\n",
        "        self.loc_emb = nn.Embedding(num_locations, 56, padding_idx=0)\n",
        "        self.user_emb = nn.Embedding(num_users, 12, padding_idx=0)\n",
        "        \n",
        "        # Compact temporal encoder\n",
        "        self.temporal_proj = nn.Linear(6, 12)  # sin/cos time, dur, sin/cos wd, gap\n",
        "        \n",
        "        # Input fusion: 56 + 12 + 12 = 80\n",
        "        self.input_norm = nn.LayerNorm(80)\n",
        "        \n",
        "        # Positional encoding\n",
        "        pe = torch.zeros(max_seq_len, 80)\n",
        "        position = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, 80, 2).float() * (-math.log(10000.0) / 80))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe)\n",
        "        \n",
        "        # Very compact transformer\n",
        "        self.attn = nn.MultiheadAttention(80, 4, dropout=0.35, batch_first=True)\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.Linear(80, 160),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(0.35),\n",
        "            nn.Linear(160, 80)\n",
        "        )\n",
        "        self.norm1 = nn.LayerNorm(80)\n",
        "        self.norm2 = nn.LayerNorm(80)\n",
        "        self.dropout = nn.Dropout(0.35)\n",
        "        \n",
        "        # Prediction head\n",
        "        self.predictor = nn.Sequential(\n",
        "            nn.Linear(80, 160),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(160, num_locations)\n",
        "        )\n",
        "        \n",
        "        # History scoring parameters (learnable)\n",
        "        self.recency_decay = nn.Parameter(torch.tensor(0.62))\n",
        "        self.freq_weight = nn.Parameter(torch.tensor(2.2))\n",
        "        self.history_scale = nn.Parameter(torch.tensor(11.0))\n",
        "        self.model_weight = nn.Parameter(torch.tensor(0.22))\n",
        "        \n",
        "        self._init_weights()\n",
        "    \n",
        "    def _init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.zeros_(m.bias)\n",
        "            elif isinstance(m, nn.Embedding):\n",
        "                nn.init.normal_(m.weight, mean=0, std=0.01)\n",
        "                if m.padding_idx is not None:\n",
        "                    m.weight.data[m.padding_idx].zero_()\n",
        "    \n",
        "    def compute_history_scores(self, loc_seq, mask):\n",
        "        \"\"\"\n",
        "        Compute history-based scores for all locations.\n",
        "        \n",
        "        Args:\n",
        "            loc_seq: (batch_size, seq_len) - location sequence\n",
        "            mask: (batch_size, seq_len) - attention mask\n",
        "        \n",
        "        Returns:\n",
        "            history_scores: (batch_size, num_locations) - scores for each location\n",
        "        \"\"\"\n",
        "        batch_size, seq_len = loc_seq.shape\n",
        "        \n",
        "        # Initialize score matrix\n",
        "        recency_scores = torch.zeros(batch_size, self.num_locations, device=loc_seq.device)\n",
        "        frequency_scores = torch.zeros(batch_size, self.num_locations, device=loc_seq.device)\n",
        "        \n",
        "        # Compute recency and frequency scores\n",
        "        for t in range(seq_len):\n",
        "            locs_t = loc_seq[:, t]  # (B,)\n",
        "            valid_t = mask[:, t].float()  # (B,)\n",
        "            \n",
        "            # Recency: exponential decay from the end\n",
        "            time_from_end = seq_len - t - 1\n",
        "            recency_weight = torch.pow(self.recency_decay, time_from_end)\n",
        "            \n",
        "            # Update recency scores (max over time for each location)\n",
        "            indices = locs_t.unsqueeze(1)  # (B, 1)\n",
        "            values = (recency_weight * valid_t).unsqueeze(1)  # (B, 1)\n",
        "            \n",
        "            # For each location, keep the maximum recency (most recent visit)\n",
        "            current_scores = torch.zeros(batch_size, self.num_locations, device=loc_seq.device)\n",
        "            current_scores.scatter_(1, indices, values)\n",
        "            recency_scores = torch.maximum(recency_scores, current_scores)\n",
        "            \n",
        "            # Update frequency scores (sum over time)\n",
        "            frequency_scores.scatter_add_(1, indices, valid_t.unsqueeze(1))\n",
        "        \n",
        "        # Normalize frequency scores\n",
        "        max_freq = frequency_scores.max(dim=1, keepdim=True)[0].clamp(min=1.0)\n",
        "        frequency_scores = frequency_scores / max_freq\n",
        "        \n",
        "        # Combine recency and frequency\n",
        "        history_scores = recency_scores + self.freq_weight * frequency_scores\n",
        "        history_scores = self.history_scale * history_scores\n",
        "        \n",
        "        return history_scores\n",
        "    \n",
        "    def forward(self, loc_seq, user_seq, weekday_seq, start_min_seq, dur_seq, diff_seq, mask):\n",
        "        batch_size, seq_len = loc_seq.shape\n",
        "        \n",
        "        # === Compute history-based scores ===\n",
        "        history_scores = self.compute_history_scores(loc_seq, mask)\n",
        "        \n",
        "        # === Learned model ===\n",
        "        # Feature extraction\n",
        "        loc_emb = self.loc_emb(loc_seq)\n",
        "        user_emb = self.user_emb(user_seq)\n",
        "        \n",
        "        # Temporal features with sinusoidal encoding\n",
        "        hours = start_min_seq / 60.0\n",
        "        time_rad = (hours / 24.0) * 2 * math.pi\n",
        "        time_sin = torch.sin(time_rad)\n",
        "        time_cos = torch.cos(time_rad)\n",
        "        \n",
        "        dur_norm = torch.log1p(dur_seq) / 8.0\n",
        "        \n",
        "        wd_rad = (weekday_seq.float() / 7.0) * 2 * math.pi\n",
        "        wd_sin = torch.sin(wd_rad)\n",
        "        wd_cos = torch.cos(wd_rad)\n",
        "        \n",
        "        diff_norm = diff_seq.float() / 7.0\n",
        "        \n",
        "        temporal_feats = torch.stack([time_sin, time_cos, dur_norm, wd_sin, wd_cos, diff_norm], dim=-1)\n",
        "        temporal_emb = self.temporal_proj(temporal_feats)\n",
        "        \n",
        "        # Combine features\n",
        "        x = torch.cat([loc_emb, user_emb, temporal_emb], dim=-1)\n",
        "        x = self.input_norm(x)\n",
        "        \n",
        "        # Add positional encoding\n",
        "        x = x + self.pe[:seq_len, :].unsqueeze(0)\n",
        "        x = self.dropout(x)\n",
        "        \n",
        "        # Transformer layer\n",
        "        attn_mask = ~mask\n",
        "        attn_out, _ = self.attn(x, x, x, key_padding_mask=attn_mask)\n",
        "        x = self.norm1(x + self.dropout(attn_out))\n",
        "        \n",
        "        ff_out = self.ff(x)\n",
        "        x = self.norm2(x + self.dropout(ff_out))\n",
        "        \n",
        "        # Get last valid position\n",
        "        seq_lens = mask.sum(dim=1) - 1\n",
        "        indices_gather = seq_lens.unsqueeze(1).unsqueeze(2).expand(batch_size, 1, self.d_model)\n",
        "        last_hidden = torch.gather(x, 1, indices_gather).squeeze(1)\n",
        "        \n",
        "        # Learned logits\n",
        "        learned_logits = self.predictor(last_hidden)\n",
        "        \n",
        "        # === Ensemble: History + Learned ===\n",
        "        # Normalize learned logits to similar scale as history scores\n",
        "        learned_logits_normalized = F.softmax(learned_logits, dim=1) * self.num_locations\n",
        "        \n",
        "        # Combine with learned weight\n",
        "        final_logits = history_scores + self.model_weight * learned_logits_normalized\n",
        "        \n",
        "        return final_logits\n",
        "    \n",
        "    def count_parameters(self):\n",
        "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
        "\n",
        "print(\"Model class defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Instantiate Model\n\nCreate the model instance and move it to the appropriate device (GPU/CPU)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device(config['system']['device'])\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "model = HistoryCentricModel(\n",
        "    num_locations=config['data']['num_locations'],\n",
        "    num_users=config['data']['num_users'],\n",
        "    max_seq_len=config['data']['max_seq_len']\n",
        ")\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "num_params = model.count_parameters()\n",
        "print(f\"\\nModel: {config['model']['name']}\")\n",
        "print(f\"Total parameters: {num_params:,}\")\n",
        "\n",
        "if num_params >= 500000:\n",
        "    print(f\"WARNING: Model has {num_params:,} parameters (limit is 500K)\")\n",
        "    print(f\"Exceeded by: {num_params - 500000:,}\")\n",
        "else:\n",
        "    print(f\"\u2713 Model is within budget (remaining: {500000 - num_params:,})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Evaluation Metrics\n",
        "\n",
        "We implement comprehensive metrics for next-location prediction:\n",
        "\n",
        "- **Accuracy@k**: Percentage of times the ground truth is in top-k predictions\n",
        "- **MRR (Mean Reciprocal Rank)**: Average of 1/rank of the correct location\n",
        "- **NDCG (Normalized Discounted Cumulative Gain)**: Ranking quality metric\n",
        "- **F1 Score**: Weighted F1 score for top-1 predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_mrr(prediction, targets):\n",
        "    \"\"\"Calculate MRR score.\"\"\"\n",
        "    index = torch.argsort(prediction, dim=-1, descending=True)\n",
        "    hits = (targets.unsqueeze(-1).expand_as(index) == index).nonzero()\n",
        "    ranks = (hits[:, -1] + 1).float()\n",
        "    rranks = torch.reciprocal(ranks)\n",
        "    return torch.sum(rranks).cpu().numpy()\n",
        "\n",
        "def get_ndcg(prediction, targets, k=10):\n",
        "    \"\"\"Calculate NDCG score.\"\"\"\n",
        "    index = torch.argsort(prediction, dim=-1, descending=True)\n",
        "    hits = (targets.unsqueeze(-1).expand_as(index) == index).nonzero()\n",
        "    ranks = (hits[:, -1] + 1).float().cpu().numpy()\n",
        "    not_considered_idx = ranks > k\n",
        "    ndcg = 1 / np.log2(ranks + 1)\n",
        "    ndcg[not_considered_idx] = 0\n",
        "    return np.sum(ndcg)\n",
        "\n",
        "def calculate_metrics(logits, true_y):\n",
        "    \"\"\"Calculate all metrics.\"\"\"\n",
        "    top1 = []\n",
        "    result_ls = []\n",
        "    \n",
        "    for k in [1, 3, 5, 10]:\n",
        "        if logits.shape[-1] < k:\n",
        "            k = logits.shape[-1]\n",
        "        prediction = torch.topk(logits, k=k, dim=-1).indices\n",
        "        if k == 1:\n",
        "            top1 = torch.squeeze(prediction).cpu()\n",
        "        top_k = torch.eq(true_y[:, None], prediction).any(dim=1).sum().cpu().numpy()\n",
        "        result_ls.append(top_k)\n",
        "    \n",
        "    result_ls.append(get_mrr(logits, true_y))\n",
        "    result_ls.append(get_ndcg(logits, true_y))\n",
        "    result_ls.append(true_y.shape[0])\n",
        "    \n",
        "    return np.array(result_ls, dtype=np.float32), true_y.cpu(), top1\n",
        "\n",
        "def get_performance_dict(metrics_dict):\n",
        "    \"\"\"Convert raw counts to percentages.\"\"\"\n",
        "    perf = {\n",
        "        'acc@1': metrics_dict['correct@1'] / metrics_dict['total'] * 100,\n",
        "        'acc@3': metrics_dict['correct@3'] / metrics_dict['total'] * 100,\n",
        "        'acc@5': metrics_dict['correct@5'] / metrics_dict['total'] * 100,\n",
        "        'acc@10': metrics_dict['correct@10'] / metrics_dict['total'] * 100,\n",
        "        'mrr': metrics_dict['rr'] / metrics_dict['total'] * 100,\n",
        "        'ndcg': metrics_dict['ndcg'] / metrics_dict['total'] * 100,\n",
        "        'f1': metrics_dict['f1'],\n",
        "        'total': metrics_dict['total']\n",
        "    }\n",
        "    return perf\n",
        "\n",
        "print(\"Metric functions defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Loss Function and Optimizer\n",
        "\n",
        "**Label Smoothing Cross-Entropy**: Instead of hard targets (one-hot), we use label smoothing to prevent overconfidence and improve generalization.\n",
        "\n",
        "**AdamW Optimizer**: With separate weight decay for different parameter groups:\n",
        "- Weights: Apply weight decay\n",
        "- Biases and layer norms: No weight decay\n",
        "\n",
        "**Learning Rate Scheduler**: ReduceLROnPlateau reduces LR when validation loss plateaus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LabelSmoothingCrossEntropy(nn.Module):\n",
        "    \"\"\"Label smoothing for better generalization.\"\"\"\n",
        "    def __init__(self, smoothing=0.1):\n",
        "        super().__init__()\n",
        "        self.smoothing = smoothing\n",
        "    \n",
        "    def forward(self, pred, target):\n",
        "        n_class = pred.size(1)\n",
        "        one_hot = torch.zeros_like(pred).scatter(1, target.unsqueeze(1), 1)\n",
        "        one_hot = one_hot * (1 - self.smoothing) + self.smoothing / n_class\n",
        "        log_prob = F.log_softmax(pred, dim=1)\n",
        "        loss = -(one_hot * log_prob).sum(dim=1).mean()\n",
        "        return loss\n",
        "\n",
        "# Loss function\n",
        "criterion = LabelSmoothingCrossEntropy(smoothing=config['training']['label_smoothing'])\n",
        "print(\"Loss function initialized\")\n",
        "\n",
        "# Optimizer with separate weight decay for different param groups\n",
        "param_groups = [\n",
        "    {'params': [p for n, p in model.named_parameters() if 'bias' not in n and 'norm' not in n], \n",
        "     'weight_decay': config['training']['weight_decay']},\n",
        "    {'params': [p for n, p in model.named_parameters() if 'bias' in n or 'norm' in n], \n",
        "     'weight_decay': 0.0}\n",
        "]\n",
        "optimizer = AdamW(param_groups, lr=config['training']['learning_rate'])\n",
        "print(\"Optimizer initialized\")\n",
        "\n",
        "# Learning rate scheduler\n",
        "scheduler = ReduceLROnPlateau(\n",
        "    optimizer,\n",
        "    mode='min',\n",
        "    factor=config['training']['lr_scheduler_factor'],\n",
        "    patience=config['training']['lr_scheduler_patience'],\n",
        "    verbose=True,\n",
        "    min_lr=config['training']['min_lr']\n",
        ")\n",
        "print(\"LR scheduler initialized\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Training and Validation Functions\n",
        "\n",
        "### Training Function:\n",
        "1. Iterate through training batches\n",
        "2. Forward pass through model\n",
        "3. Calculate loss\n",
        "4. Backward pass and gradient clipping\n",
        "5. Update parameters\n",
        "\n",
        "### Validation Function:\n",
        "1. No gradient computation (eval mode)\n",
        "2. Calculate predictions\n",
        "3. Compute all metrics (Acc@k, MRR, NDCG, F1)\n",
        "4. Return performance dictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_one_epoch(model, train_loader, criterion, optimizer, device, grad_clip, epoch):\n",
        "    \"\"\"Train for one epoch.\"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    num_batches = 0\n",
        "    \n",
        "    pbar = tqdm(train_loader, desc=f'Epoch {epoch} [Train]', leave=False)\n",
        "    \n",
        "    for batch in pbar:\n",
        "        # Move to device\n",
        "        loc_seq = batch['loc_seq'].to(device)\n",
        "        user_seq = batch['user_seq'].to(device)\n",
        "        weekday_seq = batch['weekday_seq'].to(device)\n",
        "        start_min_seq = batch['start_min_seq'].to(device)\n",
        "        dur_seq = batch['dur_seq'].to(device)\n",
        "        diff_seq = batch['diff_seq'].to(device)\n",
        "        target = batch['target'].to(device)\n",
        "        mask = batch['mask'].to(device)\n",
        "        \n",
        "        # Forward pass\n",
        "        logits = model(loc_seq, user_seq, weekday_seq, start_min_seq, dur_seq, diff_seq, mask)\n",
        "        \n",
        "        # Calculate loss\n",
        "        loss = criterion(logits, target)\n",
        "        \n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        \n",
        "        # Gradient clipping\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        total_loss += loss.item()\n",
        "        num_batches += 1\n",
        "        \n",
        "        pbar.set_postfix({'loss': f'{total_loss/num_batches:.4f}'})\n",
        "    \n",
        "    avg_loss = total_loss / num_batches\n",
        "    return avg_loss\n",
        "\n",
        "@torch.no_grad()\n",
        "def validate(model, data_loader, criterion, device, split_name='Val'):\n",
        "    \"\"\"Validate the model.\"\"\"\n",
        "    model.eval()\n",
        "    \n",
        "    metrics = {\n",
        "        'correct@1': 0,\n",
        "        'correct@3': 0,\n",
        "        'correct@5': 0,\n",
        "        'correct@10': 0,\n",
        "        'rr': 0,\n",
        "        'ndcg': 0,\n",
        "        'f1': 0,\n",
        "        'total': 0\n",
        "    }\n",
        "    \n",
        "    true_ls = []\n",
        "    top1_ls = []\n",
        "    total_val_loss = 0\n",
        "    num_batches = 0\n",
        "    \n",
        "    pbar = tqdm(data_loader, desc=f'{split_name:5s}', leave=False)\n",
        "    \n",
        "    for batch in pbar:\n",
        "        loc_seq = batch['loc_seq'].to(device)\n",
        "        user_seq = batch['user_seq'].to(device)\n",
        "        weekday_seq = batch['weekday_seq'].to(device)\n",
        "        start_min_seq = batch['start_min_seq'].to(device)\n",
        "        dur_seq = batch['dur_seq'].to(device)\n",
        "        diff_seq = batch['diff_seq'].to(device)\n",
        "        target = batch['target'].to(device)\n",
        "        mask = batch['mask'].to(device)\n",
        "        \n",
        "        logits = model(loc_seq, user_seq, weekday_seq, start_min_seq, dur_seq, diff_seq, mask)\n",
        "        \n",
        "        loss = criterion(logits, target)\n",
        "        total_val_loss += loss.item()\n",
        "        num_batches += 1\n",
        "        \n",
        "        result, batch_true, batch_top1 = calculate_metrics(logits, target)\n",
        "        \n",
        "        metrics['correct@1'] += result[0]\n",
        "        metrics['correct@3'] += result[1]\n",
        "        metrics['correct@5'] += result[2]\n",
        "        metrics['correct@10'] += result[3]\n",
        "        metrics['rr'] += result[4]\n",
        "        metrics['ndcg'] += result[5]\n",
        "        metrics['total'] += result[6]\n",
        "        \n",
        "        true_ls.extend(batch_true.tolist())\n",
        "        if not batch_top1.shape:\n",
        "            top1_ls.extend([batch_top1.tolist()])\n",
        "        else:\n",
        "            top1_ls.extend(batch_top1.tolist())\n",
        "        \n",
        "        pbar.set_postfix({'loss': f'{total_val_loss/num_batches:.4f}'})\n",
        "    \n",
        "    avg_val_loss = total_val_loss / num_batches\n",
        "    \n",
        "    # Calculate F1\n",
        "    f1 = f1_score(true_ls, top1_ls, average='weighted')\n",
        "    metrics['f1'] = f1\n",
        "    \n",
        "    perf = get_performance_dict(metrics)\n",
        "    perf['val_loss'] = avg_val_loss\n",
        "    \n",
        "    return perf\n",
        "\n",
        "print(\"Training and validation functions defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Main Training Loop\n",
        "\n",
        "The main training loop:\n",
        "1. Trains for each epoch\n",
        "2. Validates on validation set\n",
        "3. Updates learning rate based on validation loss\n",
        "4. Saves best model (lowest validation loss)\n",
        "5. Early stopping if no improvement for patience epochs\n",
        "\n",
        "**Model Selection**: We use **validation loss** (not accuracy) to select the best model, as it's more stable and generalizes better."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training state\n",
        "best_val_loss = float('inf')\n",
        "best_epoch = 0\n",
        "best_model_state = None\n",
        "epochs_without_improvement = 0\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "num_epochs = config['training']['num_epochs']\n",
        "early_stop_patience = config['training']['early_stopping_patience']\n",
        "grad_clip = config['training']['grad_clip']\n",
        "\n",
        "print(f\"Starting training on {device}\")\n",
        "print(f\"Model parameters: {num_params:,}\")\n",
        "print(f\"Using validation loss for model selection\\n\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    print(f\"\\n=== Epoch {epoch}/{num_epochs} ===\")\n",
        "    epoch_start = time.time()\n",
        "    \n",
        "    # Train\n",
        "    train_loss = train_one_epoch(model, train_loader, criterion, optimizer, device, grad_clip, epoch)\n",
        "    train_losses.append(train_loss)\n",
        "    \n",
        "    # Validate\n",
        "    val_perf = validate(model, val_loader, criterion, device, split_name='Val')\n",
        "    val_loss = val_perf['val_loss']\n",
        "    val_losses.append(val_loss)\n",
        "    \n",
        "    # Learning rate scheduling\n",
        "    scheduler.step(val_loss)\n",
        "    \n",
        "    # Display results\n",
        "    print(f\"Val - Loss: {val_loss:.4f} | Acc@1: {val_perf['acc@1']:.2f}% Acc@5: {val_perf['acc@5']:.2f}% Acc@10: {val_perf['acc@10']:.2f}% | F1: {100*val_perf['f1']:.2f}% MRR: {val_perf['mrr']:.2f}% NDCG: {val_perf['ndcg']:.2f}%\")\n",
        "    \n",
        "    # Check for improvement\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        best_epoch = epoch\n",
        "        epochs_without_improvement = 0\n",
        "        \n",
        "        # Save best model state\n",
        "        best_model_state = model.state_dict().copy()\n",
        "        \n",
        "        print(f\"\u2713 New best model! Val Loss: {val_loss:.4f} (Acc@1: {val_perf['acc@1']:.2f}%)\")\n",
        "    else:\n",
        "        epochs_without_improvement += 1\n",
        "    \n",
        "    epoch_time = time.time() - epoch_start\n",
        "    print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} (best: {best_val_loss:.4f} @ epoch {best_epoch}) | Time: {epoch_time:.1f}s\")\n",
        "    print(f\"Epochs without improvement: {epochs_without_improvement}/{early_stop_patience}\")\n",
        "    \n",
        "    # Early stopping\n",
        "    if epochs_without_improvement >= early_stop_patience:\n",
        "        print(f\"\\nEarly stopping triggered after {epoch} epochs\")\n",
        "        break\n",
        "\n",
        "total_time = time.time() - start_time\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"Training completed!\")\n",
        "print(f\"Best validation loss: {best_val_loss:.4f} at epoch {best_epoch}\")\n",
        "print(f\"Total training time: {total_time:.2f}s\")\n",
        "print(\"=\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Load Best Model and Evaluate on Test Set\n",
        "\n",
        "After training completes, we:\n",
        "1. Load the best model state (from epoch with lowest validation loss)\n",
        "2. Evaluate on the held-out test set\n",
        "3. Report comprehensive metrics\n",
        "\n",
        "This gives us an unbiased estimate of model performance on unseen data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load best model\n",
        "if best_model_state is not None:\n",
        "    model.load_state_dict(best_model_state)\n",
        "    print(f\"Loaded best model from epoch {best_epoch} (Val Loss: {best_val_loss:.4f})\")\n",
        "else:\n",
        "    print(\"Using final model state\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"EVALUATION ON TEST SET\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Evaluate on test set\n",
        "test_perf = validate(model, test_loader, criterion, device, split_name='Test')\n",
        "\n",
        "print(f\"\\nTest - Loss: {test_perf['val_loss']:.4f} | Acc@1: {test_perf['acc@1']:.2f}% Acc@5: {test_perf['acc@5']:.2f}% Acc@10: {test_perf['acc@10']:.2f}% | F1: {100*test_perf['f1']:.2f}% MRR: {test_perf['mrr']:.2f}% NDCG: {test_perf['ndcg']:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13. Final Results Summary\n",
        "\n",
        "Display all final metrics in a clear, organized format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"FINAL RESULTS\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"\\nTraining Summary:\")\n",
        "print(f\"  Best Validation Loss: {best_val_loss:.4f} (Epoch {best_epoch})\")\n",
        "print(f\"  Total Epochs Trained: {epoch}\")\n",
        "print(f\"  Training Time: {total_time:.2f}s\")\n",
        "print(f\"  Model Parameters: {num_params:,}\")\n",
        "\n",
        "print(f\"\\nTest Set Performance:\")\n",
        "print(f\"  Accuracy@1:  {test_perf['acc@1']:.2f}%\")\n",
        "print(f\"  Accuracy@3:  {test_perf['acc@3']:.2f}%\")\n",
        "print(f\"  Accuracy@5:  {test_perf['acc@5']:.2f}%\")\n",
        "print(f\"  Accuracy@10: {test_perf['acc@10']:.2f}%\")\n",
        "print(f\"  F1 Score:    {100 * test_perf['f1']:.2f}%\")\n",
        "print(f\"  MRR:         {test_perf['mrr']:.2f}%\")\n",
        "print(f\"  NDCG:        {test_perf['ndcg']:.2f}%\")\n",
        "print(f\"  Test Loss:   {test_perf['val_loss']:.4f}\")\n",
        "print(\"=\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 14. Training Visualization (Optional)\n",
        "\n",
        "Visualize the training and validation loss curves over epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    import matplotlib.pyplot as plt\n",
        "    \n",
        "    plt.figure(figsize=(12, 4))\n",
        "    \n",
        "    # Loss curves\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(train_losses, label='Train Loss', linewidth=2)\n",
        "    plt.plot(val_losses, label='Val Loss', linewidth=2)\n",
        "    plt.axvline(x=best_epoch-1, color='r', linestyle='--', label=f'Best Epoch ({best_epoch})')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Validation loss only (zoomed)\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(val_losses, label='Val Loss', linewidth=2, color='orange')\n",
        "    plt.axvline(x=best_epoch-1, color='r', linestyle='--', label=f'Best Epoch ({best_epoch})')\n",
        "    plt.axhline(y=best_val_loss, color='g', linestyle=':', label=f'Best Val Loss ({best_val_loss:.4f})')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Validation Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "except ImportError:\n",
        "    print(\"Matplotlib not available for visualization\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 15. Conclusion\n",
        "\n",
        "This notebook demonstrated the complete training pipeline for the **HistoryCentricModel** from start to finish:\n",
        "\n",
        "### What We Covered:\n",
        "\n",
        "1. **Data Loading**: Loaded GeoLife trajectory sequences from pickle files\n",
        "2. **Dataset Processing**: Handled variable-length sequences with custom collate function\n",
        "3. **Model Architecture**: Implemented the compact HistoryCentricModel combining:\n",
        "   - History-based scoring (recency + frequency)\n",
        "   - Learned transformer patterns\n",
        "   - Ensemble approach\n",
        "4. **Training Loop**: \n",
        "   - Label smoothing loss\n",
        "   - AdamW optimizer with weight decay\n",
        "   - Learning rate scheduling\n",
        "   - Early stopping\n",
        "5. **Evaluation**: Comprehensive metrics on test set\n",
        "\n",
        "### Key Results:\n",
        "- The model achieves strong performance while staying under 500K parameters\n",
        "- History-centric approach leverages the insight that most next locations are already in visit history\n",
        "- Compact transformer learns temporal patterns efficiently\n",
        "\n",
        "### Notes:\n",
        "- This notebook is **fully self-contained** and can be run independently\n",
        "- All code matches the production training script logic\n",
        "- Results should be comparable to running `train_model.py` with the same configuration\n",
        "\n",
        "---\n",
        "\n",
        "**End of Notebook**"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}