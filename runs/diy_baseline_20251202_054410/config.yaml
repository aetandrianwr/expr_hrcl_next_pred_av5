experiment:
  name: diy_baseline
  description: Baseline transformer model for DIY skip first part dataset
  dataset: diy
  version: '1.0'
data:
  dataset_name: diy
  data_dir: data/diy
  train_file: diy_transformer_7_train.pk
  val_file: diy_transformer_7_validation.pk
  test_file: diy_transformer_7_test.pk
model:
  name: HistoryCentricModel
  loc_emb_dim: 48
  user_emb_dim: 16
  weekday_emb_dim: 4
  time_emb_dim: 8
  d_model: 112
  nhead: 8
  num_layers: 4
  dim_feedforward: 224
  dropout: 0.2
training:
  batch_size: 256
  num_epochs: 120
  learning_rate: 0.001
  weight_decay: 1.0e-06
  grad_clip: 1.0
  label_smoothing: 0.05
  optimizer: adamw
  betas:
  - 0.9
  - 0.999
  eps: 1.0e-08
  scheduler:
    type: reduce_on_plateau
    patience: 10
    factor: 0.6
    min_lr: 5.0e-07
    warmup_epochs: 3
    T_max: 50
  early_stopping:
    patience: 10
    metric: val_loss
    mode: min
evaluation:
  metrics:
  - acc@1
  - acc@3
  - acc@5
  - acc@10
  - mrr
  - ndcg
  - f1
  save_predictions: false
system:
  device: cuda
  num_workers: 2
  pin_memory: true
  seed: 42
  deterministic: true
logging:
  log_interval: 50
  verbose: true
  wandb:
    enabled: false
    project: next-location-prediction
    entity: null
checkpoint:
  save_best: true
  save_last: true
  monitor: val_loss
  mode: min
paths:
  runs_dir: runs
  results_dir: results
  checkpoints_dir: results/checkpoints
  logs_dir: results/logs
  predictions_dir: results/predictions
