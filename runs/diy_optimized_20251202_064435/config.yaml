experiment:
  name: diy_optimized
  description: 2.6M params, 4 layers, optimized
  dataset: diy
  version: '2.0'
data:
  dataset_name: diy
  data_dir: data/diy
  train_file: diy_transformer_7_train.pk
  val_file: diy_transformer_7_validation.pk
  test_file: diy_transformer_7_test.pk
model:
  name: HistoryCentricModel
  loc_emb_dim: 48
  user_emb_dim: 24
  weekday_emb_dim: 4
  time_emb_dim: 8
  d_model: 96
  nhead: 8
  num_layers: 4
  dim_feedforward: 192
  dropout: 0.1
training:
  batch_size: 128
  num_epochs: 120
  learning_rate: 0.0005
  weight_decay: 1.0e-05
  grad_clip: 1.0
  label_smoothing: 0.1
  optimizer: adamw
  betas:
  - 0.9
  - 0.999
  eps: 1.0e-08
  scheduler:
    type: reduce_on_plateau
    mode: max
    factor: 0.5
    patience: 5
    verbose: true
  early_stopping:
    patience: 15
    min_delta: 0.001
paths:
  data_root: data
  runs_dir: runs
  results_dir: results
system:
  device: cuda
  seed: 42
