# Test Config with Intentional Mismatch
# This config has DIFFERENT values than the model defaults to test validation

# Experiment metadata
experiment:
  name: "test_config_validation"
  description: "Test that validation warnings work"
  dataset: "diy_skip_first_part"
  version: "1.0"

# Dataset configuration
data:
  dataset_name: "diy_skip_first_part"
  data_dir: "data/diy_skip_first_part"
  train_file: "diy_skip_first_part_transformer_7_train.pk"
  val_file: "diy_skip_first_part_transformer_7_validation.pk"
  test_file: "diy_skip_first_part_transformer_7_test.pk"

# Model architecture - INTENTIONALLY DIFFERENT FROM DEFAULTS
model:
  name: "HistoryCentricModel"
  
  # These DON'T match model defaults (should trigger warnings)
  loc_emb_dim: 128    # Model default: 56
  user_emb_dim: 32    # Model default: 12
  d_model: 256        # Model default: 80
  nhead: 8
  num_layers: 2
  dim_feedforward: 512
  dropout: 0.5

# Training configuration
training:
  batch_size: 32
  num_epochs: 2
  learning_rate: 0.001
  weight_decay: 0.0001
  grad_clip: 1.0
  label_smoothing: 0.0
  
  optimizer: "adamw"
  betas: [0.9, 0.999]
  eps: 1.0e-8
  
  scheduler:
    type: "reduce_on_plateau"
    patience: 10
    factor: 0.5
    min_lr: 1.0e-6
    warmup_epochs: 0
    T_max: 50
  
  early_stopping:
    patience: 5
    metric: "val_loss"
    mode: "min"

# System configuration
system:
  device: "auto"
  num_workers: 2
  pin_memory: true
  seed: 42
  deterministic: true

# Logging
logging:
  log_interval: 50
  verbose: true
  wandb:
    enabled: false

checkpoint:
  save_best: false
  save_last: false

# Output paths
paths:
  runs_dir: "runs"
  results_dir: "results"
